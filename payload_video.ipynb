{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fa1df42",
      "metadata": {
        "id": "7fa1df42"
      },
      "source": [
        "# FastAPI upload server (payload_video.ipynb)\n",
        "\n",
        "Notebook ini menyediakan server FastAPI yang menerima upload video (multipart) di `/upload` dan menerima JSON payload di `/upload`.\n",
        "\n",
        "Langkah eksekusi:\n",
        "1. Jalankan cell instalasi dependensi\n",
        "2. Jalankan cell setup direktori\n",
        "3. Jalankan cell definisi server\n",
        "4. Jalankan cell start server (ngrok akan dicoba jika tersedia)\n",
        "\n",
        "Hasil: file yang diupload akan disimpan di folder `uploads/` dan payload JSON yang dikirim ke `/upload` akan disimpan di `received_payloads/`. Video akan diproses dengan Whisper untuk speech-to-text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "cd864408",
      "metadata": {
        "id": "cd864408"
      },
      "outputs": [],
      "source": [
        "#kalo pake colab jangan lupa install dulu di terminal\n",
        "#!pip install --quiet numpy==1.26.4\n",
        "#!pip install --quiet --upgrade torch torchaudio faster-whisper\n",
        "\n",
        "\n",
        "#kalo lokal download ffmpeg nya\n",
        "#https://github.com/GyanD/codexffmpeg/releases/download/2025-11-27-git-61b034a47c/ffmpeg-2025-11-27-git-61b034a47c-full_build.zip\n",
        "#simpen di c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "5e2da1be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2da1be",
        "outputId": "f3d04032-2ab5-42ce-a290-f8329e6c2f2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ All safe packages installed\n",
            "   ‚úÖ No numpy version conflicts\n",
            "   ‚úÖ Jupyter widgets installed (fixes tqdm warning)\n",
            "   ‚úÖ FFmpeg required for audio - verify with next cell\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] += os.pathsep + r\"C:\\ffmpeg\\bin\"\n",
        "\n",
        "# ============================================================================\n",
        "# üîß CELL 1: INSTALL SAFE DEPENDENCIES (FIXED - NO CONFLICTS!)\n",
        "# ============================================================================\n",
        "\n",
        "# ‚úÖ TIER 0: JUPYTER WIDGETS (fixes tqdm warning)\n",
        "!pip install --quiet ipywidgets jupyter\n",
        "# ‚úÖ TIER 1: AMAN (Tidak touch numpy)\n",
        "!pip install --quiet fastapi uvicorn nest-asyncio pyngrok python-multipart\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet imageio-ffmpeg\n",
        "!pip install --quiet deepl\n",
        "\n",
        "# ‚úÖ TIER 2: AMAN (Pure torch-based, no numpy dependency)\n",
        "#!pip install --quiet torch torchaudio\n",
        "!pip install --quiet silero-vad\n",
        "\n",
        "# ‚úÖ TIER 3: AMAN (Minimal numpy, tidak upgrade)\n",
        "!pip install --quiet pydub\n",
        "!pip install --quiet soundfile\n",
        "!pip install --quiet scipy\n",
        "!pip install --quiet scikit-learn\n",
        "\n",
        "# ‚úÖ TIER 4: AMAN (Cloud-based, no local deps)\n",
        "#!pip install --quiet faster-whisper\n",
        "!pip install --quiet huggingface-hub\n",
        "\n",
        "# ‚úÖ TIER 5: MEDIAPIPE (sudah include opencv internally!)\n",
        "!pip install --quiet mediapipe\n",
        "# ‚úÖ TIER 6: TORCHCODEC (video codec support)\n",
        "!pip install --quiet torchcodec\n",
        "!pip install --quiet librosa\n",
        "\n",
        "print('\\n‚úÖ All safe packages installed')\n",
        "print('   ‚úÖ No numpy version conflicts')\n",
        "print('   ‚úÖ Jupyter widgets installed (fixes tqdm warning)')\n",
        "print('   ‚úÖ FFmpeg required for audio - verify with next cell')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBPyMnTdJnsi",
      "metadata": {
        "id": "BBPyMnTdJnsi"
      },
      "source": [
        "<b><h2> Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "3oRQo25wR_LJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oRQo25wR_LJ",
        "outputId": "ccf1e369-8255-4b4d-f183-c1991603c199"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Standard Library\n",
        "# ==========================\n",
        "import asyncio\n",
        "import gc\n",
        "import getpass\n",
        "import hashlib\n",
        "import json\n",
        "import json as json_module\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import tempfile\n",
        "import threading\n",
        "import threading as th\n",
        "import time\n",
        "import traceback\n",
        "import uuid\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime, timezone\n",
        "from typing import List\n",
        "from urllib.parse import urlparse\n",
        "import urllib.request\n",
        "import torch\n",
        "import torchaudio\n",
        "from silero_vad import load_silero_vad\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent\n",
        "\n",
        "# ==========================\n",
        "# Third-Party Libraries\n",
        "# ==========================\n",
        "import deepl\n",
        "import nest_asyncio\n",
        "import torch\n",
        "import uvicorn\n",
        "from faster_whisper import WhisperModel\n",
        "from huggingface_hub import InferenceClient\n",
        "from pyngrok import conf, ngrok\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "# ==========================\n",
        "# FastAPI & Middleware\n",
        "# ==========================\n",
        "from fastapi import (\n",
        "    BackgroundTasks,\n",
        "    FastAPI,\n",
        "    File,\n",
        "    Form,\n",
        "    HTTPException,\n",
        "    Request,\n",
        "    UploadFile\n",
        ")\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from fastapi.staticfiles import StaticFiles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sZG11EuZJsrV",
      "metadata": {
        "id": "sZG11EuZJsrV"
      },
      "source": [
        "<b><h2> Siapkan direktori untuk upload dan transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "5359c402",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5359c402",
        "outputId": "e4661b7e-2b10-4dca-a151-d1449dfa761c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Directories:\n",
            "   Upload: d:\\Interview_Assesment_System-ngrok-raifal\\uploads\n",
            "   Transcription: d:\\Interview_Assesment_System-ngrok-raifal\\transcriptions\n",
            "   AUDIO: d:\\Interview_Assesment_System-ngrok-raifal\\audio\n",
            "   Results: d:\\Interview_Assesment_System-ngrok-raifal\\results\n",
            "\n",
            "üéØ Device Configuration:\n",
            "   Device: CPU\n",
            "   Compute Type: int8\n",
            "   Note: Using CPU (GPU recommended for faster processing)\n",
            "\n",
            "üåê Translation Configuration:\n",
            "   DeepL API: Configured\n"
          ]
        }
      ],
      "source": [
        "# Siapkan direktori untuk upload dan transcription\n",
        "ROOT_DIR = os.getcwd()\n",
        "UPLOAD_DIR = os.path.join(ROOT_DIR, 'uploads')\n",
        "TRANSCRIPTION_DIR = os.path.join(ROOT_DIR, 'transcriptions')\n",
        "AUDIO_DIR = os.path.join(ROOT_DIR, 'audio')\n",
        "RESULTS_DIR = os.path.join(ROOT_DIR, 'results')  # NEW: hasil assessment\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(TRANSCRIPTION_DIR, exist_ok=True)\n",
        "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print('üìÅ Directories:')\n",
        "print(f'   Upload: {UPLOAD_DIR}')\n",
        "print(f'   Transcription: {TRANSCRIPTION_DIR}')\n",
        "print(f'   AUDIO: {AUDIO_DIR}')\n",
        "print(f'   Results: {RESULTS_DIR}')\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "print(f'\\nüéØ Device Configuration:')\n",
        "print(f'   Device: {device.upper()}')\n",
        "print(f'   Compute Type: {compute_type}')\n",
        "if device == \"cuda\":\n",
        "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    print('   Note: Using CPU (GPU recommended for faster processing)')\n",
        "\n",
        "# DeepL Configuration\n",
        "DEEPL_API_KEY = \"02a88edf-4fcb-4786-ba3d-a137fb143760:fx\"\n",
        "\n",
        "print('\\nüåê Translation Configuration:')\n",
        "print(f'   DeepL API: {\"Configured\" if DEEPL_API_KEY != \"YOUR_DEEPL_API_KEY_HERE\" else \"‚ö†Ô∏è  NOT CONFIGURED - Set DEEPL_API_KEY\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "-xKfkVrjT1Fb",
      "metadata": {
        "id": "-xKfkVrjT1Fb"
      },
      "outputs": [],
      "source": [
        "app = FastAPI(title='AI Interview Assessment System')\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        "    expose_headers=['*'],\n",
        "    max_age=3600,\n",
        ")\n",
        "\n",
        "# Mount static folders\n",
        "app.mount('/uploads', StaticFiles(directory=UPLOAD_DIR), name='uploads')\n",
        "app.mount('/transcriptions', StaticFiles(directory=TRANSCRIPTION_DIR), name='transcriptions')\n",
        "app.mount('/results', StaticFiles(directory=RESULTS_DIR), name='results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "YscyXmgL1ZMC",
      "metadata": {
        "id": "YscyXmgL1ZMC"
      },
      "outputs": [],
      "source": [
        "# Background processing\n",
        "executor = ThreadPoolExecutor(max_workers=2)\n",
        "processing_status = {}\n",
        "processing_lock = th.Lock()\n",
        "\n",
        "# HELPER FUNCTIONS - ONLY ONE INSTANCE EACH\n",
        "\n",
        "def get_local_file_path(url):\n",
        "    \"\"\"Extract local file path from URL if it's a local upload\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        if '/uploads/' in parsed.path:\n",
        "            filename = parsed.path.split('/uploads/')[-1]\n",
        "            local_path = os.path.join(UPLOAD_DIR, filename)\n",
        "            if os.path.exists(local_path):\n",
        "                return local_path\n",
        "    except Exception as e:\n",
        "        print(f'Error parsing URL: {e}')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hn4TX5r-JxtM",
      "metadata": {
        "id": "Hn4TX5r-JxtM"
      },
      "source": [
        "<b><h2> **Initialize** Whisper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "yNRfVfwTT4fC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507,
          "referenced_widgets": [
            "4cec7df9ce754d8f9071445537a2d71d",
            "32dba1a9391d474c87df9e6481d2ea77",
            "f59b66bc5298449f8e0029126486309e",
            "a69691951c614bb0a305bcf54740a454",
            "986be52f627843029cade37a9af5b821",
            "100b1834bf164f86977e85126bd92565",
            "08f44b35a30048b1bafa2d880d5ace72",
            "71159fcbc0984d438368c9c1cd607b56",
            "9aef69c2324a47cd9ac0b11ec8fbf1ef",
            "0938548727284786a6a9a46b76e4ad5d",
            "67e4d643798f4b05b3ed99fdf3437605",
            "05a63d6b2e3141b7ba80009416f089ca",
            "6f2f59cf25b548e0991783c3381b3c80",
            "0b8fc299e92445779f38301955f1c74a",
            "4a58e52a06b442f7bcf947fd1f85a73f",
            "85230263940e4be08284ca1d8d678ec7",
            "6dbfca3b9a2d41b0bf6e888f53869de7",
            "b73fb2727c6b4196a53544b470ee2e7e",
            "85706ed4d1db4286b509cce06903a6e0",
            "919039342ed9486f93800e16866b3aee",
            "7ca7487931944012bf8da019b51c49fd",
            "63ef4e4ea3c44d3da37ca1e2eb7efa08",
            "07a485be84e5483b922b56308c3d80d1",
            "d54a1e7f7ba54cb98b8d14a36a4a928f",
            "29fdf0e668fb47ef97eb1f7f09949904",
            "6a6c1b4f8f0d41c6afaf3659513c6da0",
            "5e8345a4f35e4a899db2f2e517dacd2d",
            "cad30de3a6514d1bb9af1d4147eda7b8",
            "70c21b57c03b4acfa41ff6e0e55621c7",
            "f512c6f65bc444ff9197c242a0b6fb60",
            "283fd83ed74243c0b1b62db7ce2faf94",
            "08869c60ae074093bf22243dc4100d31",
            "d52781d1e05b4442b9ac7c5b8f907dd5",
            "4f2ecdc4ac6548f0bf96e5feacd7b7f8",
            "1ca42faa03b0404c9f170fc152b3514e",
            "b48e6a73f6e345028987bbe195f0b96e",
            "32b0fd7dcc544c3e95297220fe9a5067",
            "de144b6aa6834f01aad4924324e09aae",
            "b77f6063438a44e7bc7dd9ffb86359ee",
            "f65e028e93ba4bf08fea59a6bac8293b",
            "4e047c2867f9472c94a4d17814613e3b",
            "f359ce416141466fa4d44ca97b0d99ed",
            "f3467fbff9a44b9aaceb48dac936835a",
            "251e90b4abff4b89b861f8165b6e18e9",
            "d98113b7cd014433a10886bd6743b010",
            "4e98f7ff057849fb8c299aa8cef0fb47",
            "d87be38d193b4fcb8005ad358fa1abbc",
            "ab5afedbe0334e458df8344142362444",
            "f2f1521ff5944549b95f7a092dd5692f",
            "4dc4963f0cb945bf9aebf6c08aa1596c",
            "be695d1f31b042c9a6ec11af0f6c4166",
            "3adb986c4c564adaa7ef21adde0a400d",
            "79ba8a43171f4b25831cd99ede0a898b",
            "4017fad5c27c4edd8948951420ede424",
            "69ab95865fac4d6485d43f67e5107267"
          ]
        },
        "id": "yNRfVfwTT4fC",
        "outputId": "9893d7f5-f27a-49ee-b24e-6a651b926a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì• Loading Whisper model...\n",
            "‚ÑπÔ∏è  Using faster-whisper \"large-v3\" model\n",
            "   This is the MOST ACCURATE model available\n",
            "   Speed: 4-5x faster than openai-whisper\n",
            "   Accuracy: ~98% for clear English speech\n",
            "   First run will download ~3GB model...\n",
            "\n",
            "üéØ Configuration:\n",
            "   Device: CPU\n",
            "   Compute Type: int8\n",
            "‚úÖ Whisper model loaded successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load faster-whisper model with BEST ACCURACY settings\n",
        "print('\\nüì• Loading Whisper model...')\n",
        "print('‚ÑπÔ∏è  Using faster-whisper \"large-v3\" model')\n",
        "print('   This is the MOST ACCURATE model available')\n",
        "print('   Speed: 4-5x faster than openai-whisper')\n",
        "print('   Accuracy: ~98% for clear English speech')\n",
        "print('   First run will download ~3GB model...\\n')\n",
        "\n",
        "# Detect device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "print(f'üéØ Configuration:')\n",
        "print(f'   Device: {device.upper()}')\n",
        "print(f'   Compute Type: {compute_type}')\n",
        "\n",
        "# Load model with best accuracy settings\n",
        "whisper_model = WhisperModel(\n",
        "    \"large-v3\",\n",
        "    device=device,\n",
        "    compute_type=compute_type,\n",
        "    cpu_threads=4,\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "print('‚úÖ Whisper model loaded successfully\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oTD7T12FJ7XU",
      "metadata": {
        "id": "oTD7T12FJ7XU"
      },
      "source": [
        "<b><h2> Initialize DeepL translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "QVETu4h3T6k6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVETu4h3T6k6",
        "outputId": "635f68aa-ae00-4a1a-bd8c-f2c78dea8b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ DeepL translator initialized successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize DeepL translator\n",
        "translator = None\n",
        "if DEEPL_API_KEY and DEEPL_API_KEY != \"YOUR_DEEPL_API_KEY_HERE\":\n",
        "    try:\n",
        "        translator = deepl.Translator(DEEPL_API_KEY)\n",
        "        print('‚úÖ DeepL translator initialized successfully\\n')\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è  DeepL initialization failed: {e}')\n",
        "        print('   Translation to Indonesian will be skipped\\n')\n",
        "else:\n",
        "    print('‚ö†Ô∏è  DeepL API key not configured')\n",
        "    print('   Translation to Indonesian will be skipped\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xxvmLdAdKDLT",
      "metadata": {
        "id": "xxvmLdAdKDLT"
      },
      "source": [
        "<b><h2> Fungsi Cheating Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "daccbe13",
      "metadata": {
        "id": "daccbe13"
      },
      "outputs": [],
      "source": [
        "def perform_speaker_diarization_silero(video_path):\n",
        "    \"\"\"\n",
        "    Detect multiple speakers using Silero VAD (Voice Activity Detection)\n",
        "    FIXED: Better algorithm to distinguish between natural pauses vs multiple speakers\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print('   üé§ Performing speaker diarization (Silero VAD)...')\n",
        "        # Load Silero VAD model\n",
        "        try:\n",
        "            model = load_silero_vad()\n",
        "            print('   ‚îÇ ‚úÖ Silero VAD model loaded')\n",
        "        except Exception as e:\n",
        "            print(f'   ‚îÇ ‚ö†Ô∏è  Could not load Silero VAD: {str(e)[:50]}')\n",
        "            return {\n",
        "                'is_single_speaker': True,\n",
        "                'speaker_count': 1,\n",
        "                'duration': 0,\n",
        "                'method': 'silero_vad_unavailable',\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "        # Try to load audio\n",
        "        try:\n",
        "            print('   ‚îÇ Attempting to load audio...')\n",
        "            waveform, sample_rate = torchaudio.load(video_path)\n",
        "\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                waveform = resampler(waveform)\n",
        "                sample_rate = 16000\n",
        "\n",
        "            print(f'   ‚îÇ ‚úÖ Audio loaded: {waveform.shape[0]} channels @ {sample_rate}Hz')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'   ‚îÇ ‚ö†Ô∏è  torchaudio load failed: {str(e)[:50]}')\n",
        "\n",
        "            # Fallback: Use pydub + ffmpeg\n",
        "            try:\n",
        "                print('   ‚îÇ Fallback: Using pydub to extract audio...')\n",
        "\n",
        "                audio = AudioSegment.from_file(video_path)\n",
        "                samples = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "\n",
        "                if audio.channels == 2:\n",
        "                    samples = samples.reshape((-1, 2))\n",
        "                    samples = samples.mean(axis=1)\n",
        "\n",
        "                samples = samples / 32768.0\n",
        "                waveform = torch.from_numpy(samples).unsqueeze(0)\n",
        "                sample_rate = audio.frame_rate\n",
        "\n",
        "                if sample_rate != 16000:\n",
        "                    resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                    waveform = resampler(waveform)\n",
        "                    sample_rate = 16000\n",
        "\n",
        "                print(f'   ‚îÇ ‚úÖ Audio extracted via pydub: {waveform.shape[0]} channels @ {sample_rate}Hz')\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f'   ‚îÇ ‚ö†Ô∏è  All audio loading methods failed')\n",
        "                return {\n",
        "                    'is_single_speaker': True,\n",
        "                    'speaker_count': 1,\n",
        "                    'duration': 0,\n",
        "                    'method': 'audio_loading_failed',\n",
        "                    'error': f'{str(e)[:30]} | {str(e2)[:30]}'\n",
        "                }\n",
        "\n",
        "        duration_seconds = waveform.shape[1] / sample_rate\n",
        "        print(f'   ‚îÇ ‚ÑπÔ∏è  Audio duration: {duration_seconds:.1f}s')\n",
        "\n",
        "        # Apply Silero VAD\n",
        "        print('   ‚îÇ Analyzing speech patterns...')\n",
        "\n",
        "        CHUNK_SIZE = int(sample_rate * 0.032)  # 32ms chunks\n",
        "        chunks = waveform.squeeze(0).split(CHUNK_SIZE)\n",
        "\n",
        "        speech_segments = []  # List of (start_idx, end_idx) tuples\n",
        "        current_speech_start = None\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if len(chunk) < CHUNK_SIZE:\n",
        "                chunk = torch.nn.functional.pad(chunk, (0, CHUNK_SIZE - len(chunk)))\n",
        "\n",
        "            try:\n",
        "                speech_prob = model(chunk.unsqueeze(0), sample_rate)\n",
        "                is_speech = speech_prob > 0.5\n",
        "\n",
        "                if is_speech and current_speech_start is None:\n",
        "                    # Start of speech segment\n",
        "                    current_speech_start = i\n",
        "                elif not is_speech and current_speech_start is not None:\n",
        "                    # End of speech segment\n",
        "                    speech_segments.append((current_speech_start, i))\n",
        "                    current_speech_start = None\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Close last segment if still open\n",
        "        if current_speech_start is not None:\n",
        "            speech_segments.append((current_speech_start, len(chunks)))\n",
        "\n",
        "        print(f'   ‚îÇ ‚ÑπÔ∏è  Detected {len(speech_segments)} speech segments')\n",
        "\n",
        "        # ‚úÖ FIXED: Better multiple speaker detection logic\n",
        "        # Key indicators:\n",
        "        # 1. Number of distinct speech segments (pauses > 2s indicate speaker change)\n",
        "        # 2. Average segment length (short segments = conversation, long = monologue)\n",
        "        # 3. Variance in segment lengths (varied = conversation, uniform = single speaker)\n",
        "\n",
        "        if len(speech_segments) == 0:\n",
        "            speaker_count = 1\n",
        "            confidence = 'low'\n",
        "            print(f'   ‚îÇ    ‚ö†Ô∏è  No speech segments detected')\n",
        "        else:\n",
        "            # Calculate segment statistics\n",
        "            segment_lengths = [(end - start) * 0.032 for start, end in speech_segments]  # in seconds\n",
        "            avg_segment_length = np.mean(segment_lengths)\n",
        "            segment_variance = np.var(segment_lengths)\n",
        "\n",
        "            # Calculate silence gaps between segments\n",
        "            silence_gaps = []\n",
        "            for i in range(len(speech_segments) - 1):\n",
        "                gap = (speech_segments[i+1][0] - speech_segments[i][1]) * 0.032\n",
        "                silence_gaps.append(gap)\n",
        "\n",
        "            long_pauses = sum(1 for gap in silence_gaps if gap > 2.0)  # Pauses > 2s\n",
        "\n",
        "            print(f'   ‚îÇ ‚ÑπÔ∏è  Avg segment: {avg_segment_length:.1f}s | Long pauses: {long_pauses}')\n",
        "\n",
        "            # ‚úÖ DECISION LOGIC (FIXED)\n",
        "            # Single speaker indicators:\n",
        "            # - Few long pauses (natural thinking/breathing)\n",
        "            # - Relatively uniform segment lengths\n",
        "            # - Average segment length > 3 seconds\n",
        "\n",
        "            # Multiple speaker indicators:\n",
        "            # - Many long pauses (turn-taking)\n",
        "            # - High variance in segment lengths\n",
        "            # - Many short segments (back-and-forth conversation)\n",
        "\n",
        "            if duration_seconds < 30:\n",
        "                # Short videos: likely single speaker\n",
        "                speaker_count = 1\n",
        "                confidence = 'medium'\n",
        "            elif long_pauses < 5 and avg_segment_length > 3:\n",
        "                # Few long pauses + long segments = single speaker monologue\n",
        "                speaker_count = 1\n",
        "                confidence = 'high'\n",
        "            elif long_pauses > 15 and avg_segment_length < 2:\n",
        "                # Many pauses + short segments = conversation\n",
        "                speaker_count = 2\n",
        "                confidence = 'high'\n",
        "            elif len(speech_segments) > 30 and segment_variance > 5:\n",
        "                # Many varied segments = possible conversation\n",
        "                speaker_count = 2\n",
        "                confidence = 'medium'\n",
        "            else:\n",
        "                # Default: assume single speaker\n",
        "                speaker_count = 1\n",
        "                confidence = 'medium'\n",
        "\n",
        "            is_single_speaker = (speaker_count == 1)\n",
        "\n",
        "            print(f'   ‚îÇ ‚úÖ Analysis complete: {speaker_count} speaker(s)')\n",
        "            print(f'   ‚îÇ    Confidence: {confidence.upper()}')\n",
        "            print(f'   ‚îÇ    Reasoning: {\"Monologue pattern\" if speaker_count == 1 else \"Conversation pattern\"}')\n",
        "\n",
        "        return {\n",
        "            'is_single_speaker': is_single_speaker,\n",
        "            'speaker_count': speaker_count,\n",
        "            'duration': round(duration_seconds, 2),\n",
        "            'speech_segments': len(speech_segments),\n",
        "            'avg_segment_length': round(avg_segment_length, 2) if len(speech_segments) > 0 else 0,\n",
        "            'long_pauses': long_pauses if len(speech_segments) > 0 else 0,\n",
        "            'method': 'silero_vad_fixed',\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ö†Ô∏è  Silero VAD error: {str(e)}')\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return {\n",
        "            'is_single_speaker': True,\n",
        "            'speaker_count': 1,\n",
        "            'error': str(e),\n",
        "            'method': 'silero_vad_exception'\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "ID4Y6BXpLN8B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID4Y6BXpLN8B",
        "outputId": "474afb37-c0f8-40ac-d288-83596f76548b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Eye detection function loaded (Fixed)\n"
          ]
        }
      ],
      "source": [
        "def detect_eyes_in_video(video_path, sample_rate=5):\n",
        "    \"\"\"Detect eyes using MediaPipe - FIXED for compatibility\"\"\"\n",
        "    try:\n",
        "        # ‚úÖ OpenCV dari MediaPipe (sudah compatible)\n",
        "        print('   üëÅÔ∏è  Eye detection analysis...')\n",
        "\n",
        "        # MediaPipe solutions\n",
        "        mp_face_detection = mp.solutions.face_detection\n",
        "\n",
        "        # Open video dengan OpenCV\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print('   ‚ö†Ô∏è  Could not open video file')\n",
        "            return {\n",
        "                'is_suspicious': False,\n",
        "                'error': 'Video could not be opened',\n",
        "                'message': 'Eye detection failed - video read error'\n",
        "            }\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        if fps == 0 or fps > 120:\n",
        "            fps = 30  # Fallback\n",
        "\n",
        "        if total_frames == 0:\n",
        "            cap.release()\n",
        "            print('   ‚ö†Ô∏è  Could not determine total frames')\n",
        "            return {\n",
        "                'is_suspicious': False,\n",
        "                'error': 'Could not determine frame count',\n",
        "                'message': 'Eye detection skipped - frame count unknown'\n",
        "            }\n",
        "\n",
        "        frame_count = 0\n",
        "        eye_detected_frames = 0\n",
        "        eyes_open_frames = 0\n",
        "        eyes_closed_frames = 0\n",
        "        suspicious_frames = 0\n",
        "\n",
        "        sample_interval = max(1, int(fps / sample_rate))\n",
        "\n",
        "        print(f'   ‚îÇ FPS: {fps:.1f} | Total Frames: {total_frames} | Interval: {sample_interval}')\n",
        "\n",
        "        try:\n",
        "            # ‚úÖ MediaPipe FaceDetection\n",
        "            with mp_face_detection.FaceDetection(\n",
        "                model_selection=0,\n",
        "                min_detection_confidence=0.5\n",
        "            ) as face_detection:\n",
        "\n",
        "                while cap.isOpened():\n",
        "                    ret, frame = cap.read()\n",
        "                    if not ret:\n",
        "                        break\n",
        "\n",
        "                    if frame_count % sample_interval != 0:\n",
        "                        frame_count += 1\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        # Convert BGR to RGB\n",
        "                        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                        # Detect faces\n",
        "                        results = face_detection.process(rgb_frame)\n",
        "\n",
        "                        if results.detections:\n",
        "                            eye_detected_frames += 1\n",
        "\n",
        "                            for detection in results.detections:\n",
        "                                bbox = detection.location_data.relative_bounding_box\n",
        "                                h, w, c = frame.shape\n",
        "\n",
        "                                # Face position\n",
        "                                face_center_y = (bbox.ymin + bbox.height) * h\n",
        "\n",
        "                                # Check if looking down (suspicious)\n",
        "                                if face_center_y > h * 0.6:\n",
        "                                    suspicious_frames += 1\n",
        "\n",
        "                                # Check eyes visibility\n",
        "                                # MediaPipe detects 6 keypoints (left eye, right eye, nose, mouth, etc)\n",
        "                                if len(detection.location_data.relative_keypoints) >= 2:\n",
        "                                    eyes_open_frames += 1\n",
        "                                else:\n",
        "                                    eyes_closed_frames += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f'   ‚îÇ ‚ö†Ô∏è  Frame {frame_count} error: {str(e)[:40]}')\n",
        "                        continue\n",
        "\n",
        "                    frame_count += 1\n",
        "\n",
        "                    # Progress update\n",
        "                    if frame_count % (sample_interval * 30) == 0 and total_frames > 0:\n",
        "                        progress = (frame_count / total_frames) * 100\n",
        "                        print(f'   ‚îÇ ‚è≥ Processing: {progress:.1f}%', end='\\r')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'   ‚ö†Ô∏è  Face detection error: {str(e)}')\n",
        "\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n",
        "        # Calculate statistics\n",
        "        total_sampled_frames = frame_count\n",
        "        face_detection_rate = (eye_detected_frames / total_sampled_frames * 100) if total_sampled_frames > 0 else 0\n",
        "        suspicious_rate = (suspicious_frames / eye_detected_frames * 100) if eye_detected_frames > 0 else 0\n",
        "\n",
        "        print(f'\\n   ‚úÖ Eye Detection Complete:')\n",
        "        print(f'      Face: {face_detection_rate:.1f}% | Eyes open: {eyes_open_frames} | Eyes closed: {eyes_closed_frames}')\n",
        "\n",
        "        # Determine if suspicious\n",
        "        is_suspicious = False\n",
        "        suspicious_reasons = []\n",
        "\n",
        "        if face_detection_rate < 50:\n",
        "            is_suspicious = True\n",
        "            suspicious_reasons.append(\"Face not consistently visible\")\n",
        "\n",
        "        if suspicious_rate > 30:\n",
        "            is_suspicious = True\n",
        "            suspicious_reasons.append(\"Frequent downward gaze (reading)\")\n",
        "\n",
        "        if eyes_closed_frames > eyes_open_frames and eyes_open_frames > 0:\n",
        "            is_suspicious = True\n",
        "            suspicious_reasons.append(\"Eyes frequently closed\")\n",
        "\n",
        "        return {\n",
        "            'face_detection_rate': round(face_detection_rate, 2),\n",
        "            'eyes_open_frames': eyes_open_frames,\n",
        "            'eyes_closed_frames': eyes_closed_frames,\n",
        "            'suspicious_gaze_rate': round(suspicious_rate, 2),\n",
        "            'is_suspicious': is_suspicious,\n",
        "            'suspicious_reasons': suspicious_reasons,\n",
        "            'total_frames_analyzed': total_sampled_frames\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ö†Ô∏è  Eye detection error: {str(e)}')\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return {\n",
        "            'is_suspicious': False,\n",
        "            'error': str(e),\n",
        "            'message': 'Eye detection failed - using conservative assessment'\n",
        "        }\n",
        "\n",
        "print('‚úÖ Eye detection function loaded (Fixed)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "c04db46a",
      "metadata": {
        "id": "c04db46a"
      },
      "outputs": [],
      "source": [
        "def advanced_cheating_detection(video_path, transcription_text):\n",
        "    \"\"\"‚úÖ FIXED: Proper cheating score calculation with baseline\"\"\"\n",
        "    try:\n",
        "        print('   üö® Advanced Cheating Detection:')\n",
        "\n",
        "        cheating_indicators = []\n",
        "        cheating_score = 100  # ‚úÖ START at 100 (assume clean), DEDUCT for suspicious behavior\n",
        "        \n",
        "        confidence_components = {\n",
        "            'diarization_confidence': 0,\n",
        "            'diarization_data_quality': 0,\n",
        "            'eye_detection_confidence': 0,\n",
        "            'eye_detection_coverage': 0,\n",
        "            'text_pattern_confidence': 0,\n",
        "            'text_pattern_diversity': 0,\n",
        "            'audio_quality_confidence': 0,\n",
        "            'audio_snr': 0\n",
        "        }\n",
        "        \n",
        "        total_checks = 4\n",
        "\n",
        "        # ============================================================\n",
        "        # 1Ô∏è‚É£ DIARIZATION CHECK\n",
        "        # ============================================================\n",
        "        print('   ‚îÇ 1Ô∏è‚É£  Speaker Diarization Check')\n",
        "        diar_result = perform_speaker_diarization_silero(video_path)\n",
        "        \n",
        "        if 'confidence' in diar_result:\n",
        "            conf_map = {'high': 90, 'medium': 70, 'low': 50}\n",
        "            base_conf = conf_map.get(diar_result['confidence'], 50)\n",
        "            \n",
        "            duration = diar_result.get('duration', 0)\n",
        "            speech_segments = diar_result.get('speech_segments', 0)\n",
        "            avg_segment_length = diar_result.get('avg_segment_length', 0)\n",
        "            \n",
        "            data_quality = 50\n",
        "            if duration > 10:\n",
        "                data_quality += 20\n",
        "            if speech_segments > 5:\n",
        "                data_quality += 15\n",
        "            if avg_segment_length > 2:\n",
        "                data_quality += 15\n",
        "            \n",
        "            confidence_components['diarization_confidence'] = int(\n",
        "                (base_conf * 0.7) + (data_quality * 0.3)\n",
        "            )\n",
        "            confidence_components['diarization_data_quality'] = data_quality\n",
        "            \n",
        "            print(f'   ‚îÇ    üìä Diarization: {confidence_components[\"diarization_confidence\"]}% (base: {base_conf}, quality: {data_quality})')\n",
        "        else:\n",
        "            confidence_components['diarization_confidence'] = 50\n",
        "            confidence_components['diarization_data_quality'] = 30\n",
        "\n",
        "        # ‚úÖ DEDUCT score if multiple speakers detected\n",
        "        if not diar_result.get('is_single_speaker', True):\n",
        "            cheating_indicators.append(\n",
        "                f\"Multiple speakers detected ({diar_result.get('speaker_count', 2)} speakers)\"\n",
        "            )\n",
        "            cheating_score -= 40  # ‚úÖ DEDUCT from 100\n",
        "            print(f'   ‚îÇ    ‚ö†Ô∏è  Multiple speakers: {diar_result.get(\"speaker_count\", 2)} (-40 points)')\n",
        "        else:\n",
        "            print(f'   ‚îÇ    ‚úÖ Single speaker confirmed')\n",
        "\n",
        "        # ============================================================\n",
        "        # 2Ô∏è‚É£ EYE DETECTION CHECK\n",
        "        # ============================================================\n",
        "        print('   ‚îÇ 2Ô∏è‚É£  Eye Detection & Gaze Analysis')\n",
        "        eye_result = detect_eyes_in_video(video_path, sample_rate=5)\n",
        "        \n",
        "        if 'face_detection_rate' in eye_result:\n",
        "            face_rate = eye_result['face_detection_rate']\n",
        "            frames_analyzed = eye_result.get('total_frames_analyzed', 0)\n",
        "            eyes_open = eye_result.get('eyes_open_frames', 0)\n",
        "            \n",
        "            if face_rate > 90:\n",
        "                base_eye_conf = 95\n",
        "            elif face_rate > 75:\n",
        "                base_eye_conf = 85\n",
        "            elif face_rate > 60:\n",
        "                base_eye_conf = 75\n",
        "            elif face_rate > 45:\n",
        "                base_eye_conf = 65\n",
        "            elif face_rate > 30:\n",
        "                base_eye_conf = 55\n",
        "            else:\n",
        "                base_eye_conf = 40\n",
        "            \n",
        "            coverage_quality = min(100, (frames_analyzed / 300) * 100)\n",
        "            \n",
        "            visibility_quality = 50\n",
        "            if eyes_open > 100:\n",
        "                visibility_quality = 90\n",
        "            elif eyes_open > 50:\n",
        "                visibility_quality = 75\n",
        "            elif eyes_open > 20:\n",
        "                visibility_quality = 60\n",
        "            \n",
        "            confidence_components['eye_detection_confidence'] = int(\n",
        "                (base_eye_conf * 0.5) + (coverage_quality * 0.25) + (visibility_quality * 0.25)\n",
        "            )\n",
        "            confidence_components['eye_detection_coverage'] = int(coverage_quality)\n",
        "            \n",
        "            print(f'   ‚îÇ    üìä Eye Detection: {confidence_components[\"eye_detection_confidence\"]}% (base: {base_eye_conf}, coverage: {coverage_quality:.0f}, visibility: {visibility_quality})')\n",
        "        else:\n",
        "            confidence_components['eye_detection_confidence'] = 50\n",
        "            confidence_components['eye_detection_coverage'] = 30\n",
        "\n",
        "        # ‚úÖ DEDUCT score for suspicious eye behavior\n",
        "        if eye_result.get('is_suspicious'):\n",
        "            suspicious_count = 0\n",
        "            \n",
        "            if eye_result.get('face_detection_rate', 100) < 30:\n",
        "                cheating_indicators.append(\"Eye detection: Very low face visibility\")\n",
        "                suspicious_count += 1\n",
        "                cheating_score -= 15  # ‚úÖ DEDUCT\n",
        "                print(f'   ‚îÇ    ‚ö†Ô∏è  Low face visibility (-15 points)')\n",
        "            \n",
        "            if eye_result.get('suspicious_gaze_rate', 0) > 50:\n",
        "                cheating_indicators.append(\"Eye detection: Frequent downward gaze\")\n",
        "                suspicious_count += 1\n",
        "                cheating_score -= 15  # ‚úÖ DEDUCT\n",
        "                print(f'   ‚îÇ    ‚ö†Ô∏è  Downward gaze (-15 points)')\n",
        "            \n",
        "            if suspicious_count == 0:\n",
        "                print(f'   ‚îÇ    ‚úÖ Eye gaze analysis normal')\n",
        "        else:\n",
        "            print(f'   ‚îÇ    ‚úÖ Eye gaze analysis normal')\n",
        "\n",
        "        # ============================================================\n",
        "        # 3Ô∏è‚É£ TEXT PATTERN CHECK\n",
        "        # ============================================================\n",
        "        print('   ‚îÇ 3Ô∏è‚É£  Text Pattern Analysis')\n",
        "        words = transcription_text.split()\n",
        "        word_count = len(words)\n",
        "        \n",
        "        unique_words = len(set(word.lower() for word in words))\n",
        "        repetition_ratio = (len(words) - unique_words) / len(words) if words else 1\n",
        "        \n",
        "        if word_count >= 100:\n",
        "            base_text_conf = 95\n",
        "        elif word_count >= 50:\n",
        "            base_text_conf = 85\n",
        "        elif word_count >= 30:\n",
        "            base_text_conf = 75\n",
        "        elif word_count >= 20:\n",
        "            base_text_conf = 65\n",
        "        elif word_count >= 10:\n",
        "            base_text_conf = 55\n",
        "        elif word_count >= 5:\n",
        "            base_text_conf = 45\n",
        "        else:\n",
        "            base_text_conf = 30\n",
        "        \n",
        "        diversity_score = int((1 - repetition_ratio) * 100)\n",
        "        \n",
        "        confidence_components['text_pattern_confidence'] = int(\n",
        "            (base_text_conf * 0.6) + (diversity_score * 0.4)\n",
        "        )\n",
        "        confidence_components['text_pattern_diversity'] = diversity_score\n",
        "        \n",
        "        print(f'   ‚îÇ    üìä Text Pattern: {confidence_components[\"text_pattern_confidence\"]}% (base: {base_text_conf}, diversity: {diversity_score})')\n",
        "\n",
        "        # ‚úÖ DEDUCT score for suspicious text patterns\n",
        "        if len(words) < 3:\n",
        "            cheating_indicators.append(\"Answer extremely short (possible AI generation)\")\n",
        "            cheating_score -= 20  # ‚úÖ DEDUCT\n",
        "            print(f'   ‚îÇ    ‚ö†Ô∏è  Extremely short answer: {len(words)} words (-20 points)')\n",
        "\n",
        "        if repetition_ratio > 0.65:\n",
        "            cheating_indicators.append(f\"Very high word repetition ({repetition_ratio*100:.1f}%)\")\n",
        "            cheating_score -= 15  # ‚úÖ DEDUCT\n",
        "            print(f'   ‚îÇ    ‚ö†Ô∏è  High repetition rate: {repetition_ratio*100:.1f}% (-15 points)')\n",
        "        else:\n",
        "            print(f'   ‚îÇ    ‚úÖ Text pattern normal')\n",
        "\n",
        "        # ============================================================\n",
        "        # 4Ô∏è‚É£ AUDIO QUALITY CHECK\n",
        "        # ============================================================\n",
        "        print('   ‚îÇ 4Ô∏è‚É£  Audio Quality Check')\n",
        "        try:\n",
        "            import librosa\n",
        "            import numpy as np\n",
        "            y, sr = librosa.load(video_path, sr=16000, duration=30)\n",
        "\n",
        "            S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            noise_level = np.mean(S)\n",
        "            signal_level = np.max(S)\n",
        "            snr = signal_level / (noise_level + 1e-10)\n",
        "            \n",
        "            if snr > 50:\n",
        "                base_audio_conf = 95\n",
        "            elif snr > 30:\n",
        "                base_audio_conf = 85\n",
        "            elif snr > 20:\n",
        "                base_audio_conf = 75\n",
        "            elif snr > 10:\n",
        "                base_audio_conf = 65\n",
        "            else:\n",
        "                base_audio_conf = 50\n",
        "            \n",
        "            if noise_level < 20:\n",
        "                noise_penalty = 0\n",
        "            elif noise_level < 40:\n",
        "                noise_penalty = 10\n",
        "            elif noise_level < 60:\n",
        "                noise_penalty = 20\n",
        "            else:\n",
        "                noise_penalty = 30\n",
        "            \n",
        "            final_audio_conf = max(30, base_audio_conf - noise_penalty)\n",
        "            \n",
        "            confidence_components['audio_quality_confidence'] = int(final_audio_conf)\n",
        "            confidence_components['audio_snr'] = int(min(100, snr))\n",
        "            \n",
        "            print(f'   ‚îÇ    üìä Audio Quality: {final_audio_conf}% (SNR: {snr:.1f}, noise: {noise_level:.1f})')\n",
        "\n",
        "            # ‚úÖ DEDUCT score for high noise\n",
        "            if noise_level > 80:\n",
        "                cheating_indicators.append(f\"Very high background noise detected\")\n",
        "                cheating_score -= 10  # ‚úÖ DEDUCT\n",
        "                print(f'   ‚îÇ    ‚ö†Ô∏è  High noise level: {noise_level:.1f} (-10 points)')\n",
        "            else:\n",
        "                print(f'   ‚îÇ    ‚úÖ Audio quality normal (noise: {noise_level:.1f})')\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f'   ‚îÇ    ‚ÑπÔ∏è  Audio analysis skipped: {str(e)}')\n",
        "            fallback_audio = 50 + min(20, word_count // 5)\n",
        "            confidence_components['audio_quality_confidence'] = fallback_audio\n",
        "            confidence_components['audio_snr'] = 30\n",
        "\n",
        "        # ============================================================\n",
        "        # ‚úÖ FINALIZE CHEATING SCORE (ensure 0-100 range)\n",
        "        # ============================================================\n",
        "        cheating_score = max(0, min(100, cheating_score))\n",
        "        \n",
        "        # ‚úÖ INVERT score: High score = High cheating risk\n",
        "        # Current: 100 (clean) ‚Üí Want: 0 (clean)\n",
        "        cheating_score = 100 - cheating_score  # ‚úÖ INVERT!\n",
        "        \n",
        "        # ============================================================\n",
        "        # CALCULATE OVERALL CONFIDENCE SCORE\n",
        "        # ============================================================\n",
        "        weighted_confidence = (\n",
        "            confidence_components['diarization_confidence'] * 0.25 +\n",
        "            confidence_components['eye_detection_confidence'] * 0.25 +\n",
        "            confidence_components['text_pattern_confidence'] * 0.25 +\n",
        "            confidence_components['audio_quality_confidence'] * 0.25\n",
        "        )\n",
        "        \n",
        "        quality_adjustment = (\n",
        "            confidence_components['diarization_data_quality'] * 0.1 +\n",
        "            confidence_components['eye_detection_coverage'] * 0.1 +\n",
        "            confidence_components['text_pattern_diversity'] * 0.1 +\n",
        "            confidence_components['audio_snr'] * 0.1\n",
        "        ) / 4\n",
        "        \n",
        "        overall_confidence = min(100, weighted_confidence + quality_adjustment)\n",
        "        \n",
        "        if overall_confidence >= 85:\n",
        "            confidence_level = \"Very High\"\n",
        "        elif overall_confidence >= 75:\n",
        "            confidence_level = \"High\"\n",
        "        elif overall_confidence >= 60:\n",
        "            confidence_level = \"Medium\"\n",
        "        elif overall_confidence >= 45:\n",
        "            confidence_level = \"Low\"\n",
        "        else:\n",
        "            confidence_level = \"Very Low\"\n",
        "\n",
        "        # ‚úÖ Determine cheating status (FIXED thresholds)\n",
        "        is_cheating = cheating_score > 40  # ‚úÖ Lower threshold (was 60)\n",
        "        cheating_status = \"Ya\" if is_cheating else \"Tidak\"\n",
        "\n",
        "        print(f'   ‚îÇ üìä Final Cheating Score: {cheating_score}/100')\n",
        "        print(f'   ‚îÇ üéØ Overall Confidence: {overall_confidence:.1f}% ({confidence_level})')\n",
        "        print(f'   ‚îÇ üö® Cheating Detection: {cheating_status}')\n",
        "\n",
        "        if cheating_indicators:\n",
        "            print(f'   ‚îÇ ‚ö†Ô∏è  Indicators ({len(cheating_indicators)}):')\n",
        "            for indicator in cheating_indicators:\n",
        "                print(f'   ‚îÇ    - {indicator}')\n",
        "        else:\n",
        "            print(f'   ‚îÇ ‚úÖ No suspicious indicators found')\n",
        "\n",
        "        return {\n",
        "            'is_cheating': is_cheating,\n",
        "            'cheating_status': cheating_status,\n",
        "            'cheating_score': cheating_score,\n",
        "            'indicators': cheating_indicators,\n",
        "            'confidence_score': round(overall_confidence, 2),\n",
        "            'confidence_level': confidence_level,\n",
        "            'confidence_components': confidence_components,\n",
        "            'details': {\n",
        "                'diarization': diar_result,\n",
        "                'eye_detection': eye_result,\n",
        "                'word_count': len(words),\n",
        "                'repetition_ratio': round(repetition_ratio, 3),\n",
        "                'unique_words': unique_words,\n",
        "                'diversity_score': diversity_score\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ö†Ô∏è  Cheating detection error: {str(e)}')\n",
        "        return {\n",
        "            'is_cheating': False,\n",
        "            'cheating_status': 'Tidak',\n",
        "            'cheating_score': 0,\n",
        "            'indicators': [],\n",
        "            'confidence_score': 0,\n",
        "            'confidence_level': 'N/A',\n",
        "            'confidence_components': {},\n",
        "            'error': str(e)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "462b9730",
      "metadata": {
        "id": "462b9730"
      },
      "outputs": [],
      "source": [
        "def calculate_aggregate_cheating_analysis(assessment_results):\n",
        "    \"\"\"Enhanced aggregate analysis with MORE LENIENT thresholds\"\"\"\n",
        "    if not assessment_results:\n",
        "        return {\n",
        "            \"overall_cheating_status\": \"Tidak\",\n",
        "            \"overall_cheating_score\": 0,\n",
        "            \"total_videos\": 0,\n",
        "            \"videos_flagged\": 0,\n",
        "            \"confidence_level\": \"N/A\",\n",
        "            \"average_confidence_score\": 0\n",
        "        }\n",
        "\n",
        "    total_videos = len(assessment_results)\n",
        "    cheating_scores = []\n",
        "    confidence_scores = []\n",
        "    videos_flagged = 0\n",
        "    flagged_video_ids = []\n",
        "    cheating_indicators_summary = {}\n",
        "\n",
        "    for video in assessment_results:\n",
        "        result = video.get(\"result\", {})\n",
        "\n",
        "        cheating_score = result.get(\"cheating_score\", 0)\n",
        "        cheating_scores.append(cheating_score)\n",
        "        \n",
        "        confidence_score = result.get(\"cheating_confidence_score\", 0)\n",
        "        confidence_scores.append(confidence_score)\n",
        "\n",
        "        if result.get(\"cheating_detection\") == \"Ya\":\n",
        "            videos_flagged += 1\n",
        "            flagged_video_ids.append(video.get(\"id\"))\n",
        "\n",
        "            indicators = result.get(\"cheating_details\", {}).get(\"diarization\", {})\n",
        "            if not indicators.get(\"is_single_speaker\", True):\n",
        "                cheating_indicators_summary[\"multiple_speakers\"] = \\\n",
        "                    cheating_indicators_summary.get(\"multiple_speakers\", 0) + 1\n",
        "\n",
        "            eye_data = result.get(\"cheating_details\", {}).get(\"eye_detection\", {})\n",
        "            if eye_data.get(\"is_suspicious\", False):\n",
        "                cheating_indicators_summary[\"suspicious_eye_behavior\"] = \\\n",
        "                    cheating_indicators_summary.get(\"suspicious_eye_behavior\", 0) + 1\n",
        "\n",
        "    avg_cheating_score = sum(cheating_scores) / total_videos if total_videos > 0 else 0\n",
        "    avg_confidence_score = sum(confidence_scores) / total_videos if total_videos > 0 else 0\n",
        "    max_cheating_score = max(cheating_scores) if cheating_scores else 0\n",
        "    flagged_percentage = (videos_flagged / total_videos * 100) if total_videos > 0 else 0\n",
        "\n",
        "    if avg_confidence_score >= 85:\n",
        "        overall_confidence_level = \"Very High\"\n",
        "    elif avg_confidence_score >= 75:\n",
        "        overall_confidence_level = \"High\"\n",
        "    elif avg_confidence_score >= 60:\n",
        "        overall_confidence_level = \"Medium\"\n",
        "    elif avg_confidence_score >= 45:\n",
        "        overall_confidence_level = \"Low\"\n",
        "    else:\n",
        "        overall_confidence_level = \"Very Low\"\n",
        "\n",
        "    # ‚úÖ FIXED: More lenient decision thresholds\n",
        "    if flagged_percentage >= 70 or avg_cheating_score > 65 or max_cheating_score > 80:  # Changed from 50/50/70\n",
        "        overall_status = \"Ya\"\n",
        "        confidence = \"High\"\n",
        "        risk_level = \"HIGH RISK\"\n",
        "        recommendation = \"TIDAK LULUS - Strong evidence of cheating\"\n",
        "    elif flagged_percentage >= 50 or avg_cheating_score >= 50:  # Changed from 30/30\n",
        "        overall_status = \"Ya\"\n",
        "        confidence = \"Medium\"\n",
        "        risk_level = \"MEDIUM RISK\"\n",
        "        recommendation = \"PERTIMBANGAN - Suspicious patterns detected\"\n",
        "    else:\n",
        "        overall_status = \"Tidak\"\n",
        "        confidence = \"High\" if flagged_percentage == 0 else \"Medium\"\n",
        "        risk_level = \"LOW RISK\"\n",
        "        recommendation = \"LULUS - No significant cheating indicators\"\n",
        "\n",
        "    # ‚úÖ FIXED: Don't show specific video IDs in summary (for web display)\n",
        "    if videos_flagged > 0:\n",
        "        summary_text = f\"{videos_flagged}/{total_videos} video(s) menunjukkan indikasi kecurangan ({flagged_percentage:.1f}%). \"\n",
        "        \n",
        "        if \"multiple_speakers\" in cheating_indicators_summary:\n",
        "            count = cheating_indicators_summary[\"multiple_speakers\"]\n",
        "            summary_text += f\"Terdeteksi multiple speakers di {count} video. \"\n",
        "\n",
        "        if \"suspicious_eye_behavior\" in cheating_indicators_summary:\n",
        "            count = cheating_indicators_summary[\"suspicious_eye_behavior\"]\n",
        "            summary_text += f\"Perilaku mata mencurigakan di {count} video. \"\n",
        "    else:\n",
        "        summary_text = \"Tidak ditemukan indikasi kecurangan yang signifikan di semua video.\"\n",
        "\n",
        "    return {\n",
        "        \"overall_cheating_status\": overall_status,\n",
        "        \"overall_cheating_score\": round(avg_cheating_score, 2),\n",
        "        \"average_confidence_score\": round(avg_confidence_score, 2),\n",
        "        \"overall_confidence_level\": overall_confidence_level,\n",
        "        \"max_cheating_score\": max_cheating_score,\n",
        "        \"total_videos\": total_videos,\n",
        "        \"videos_flagged\": videos_flagged,\n",
        "        # ‚úÖ REMOVED: Don't include flagged_video_ids for web display\n",
        "        # \"flagged_video_ids\": flagged_video_ids,  \n",
        "        \"flagged_percentage\": round(flagged_percentage, 2),\n",
        "        \"confidence_level\": confidence,\n",
        "        \"risk_level\": risk_level,\n",
        "        \"recommendation\": recommendation,\n",
        "        \"summary\": summary_text,\n",
        "        \"pattern_analysis\": {\n",
        "            \"multiple_speakers_count\": cheating_indicators_summary.get(\"multiple_speakers\", 0),\n",
        "            \"suspicious_eye_behavior_count\": cheating_indicators_summary.get(\"suspicious_eye_behavior\", 0),\n",
        "            \"avg_score_per_video\": round(avg_cheating_score, 2)\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "858faa1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_confidence_improvement_tips(confidence_components):\n",
        "    \"\"\"\n",
        "    Provides actionable tips to improve confidence score\n",
        "    \"\"\"\n",
        "    tips = []\n",
        "    \n",
        "    diar = confidence_components.get('diarization_confidence', 0)\n",
        "    eye = confidence_components.get('eye_detection_confidence', 0)\n",
        "    text = confidence_components.get('text_pattern_confidence', 0)\n",
        "    audio = confidence_components.get('audio_quality_confidence', 0)\n",
        "    \n",
        "    if diar < 80:\n",
        "        tips.append({\n",
        "            'component': 'Speaker Detection',\n",
        "            'current': f'{diar:.1f}%',\n",
        "            'tips': [\n",
        "                '‚úÖ Record in quiet environment',\n",
        "                '‚úÖ Ensure only one person speaks',\n",
        "                '‚úÖ Avoid background conversations'\n",
        "            ]\n",
        "        })\n",
        "    \n",
        "    if eye < 80:\n",
        "        tips.append({\n",
        "            'component': 'Eye Detection',\n",
        "            'current': f'{eye:.1f}%',\n",
        "            'tips': [\n",
        "                '‚úÖ Position camera at eye level',\n",
        "                '‚úÖ Good lighting on face',\n",
        "                '‚úÖ Look at camera frequently',\n",
        "                '‚úÖ Avoid reading from notes'\n",
        "            ]\n",
        "        })\n",
        "    \n",
        "    if text < 80:\n",
        "        tips.append({\n",
        "            'component': 'Text Pattern',\n",
        "            'current': f'{text:.1f}%',\n",
        "            'tips': [\n",
        "                '‚úÖ Speak more (aim for 50+ words)',\n",
        "                '‚úÖ Use varied vocabulary',\n",
        "                '‚úÖ Avoid repeating same words',\n",
        "                '‚úÖ Speak clearly and naturally'\n",
        "            ]\n",
        "        })\n",
        "    \n",
        "    if audio < 80:\n",
        "        tips.append({\n",
        "            'component': 'Audio Quality',\n",
        "            'current': f'{audio:.1f}%',\n",
        "            'tips': [\n",
        "                '‚úÖ Use good microphone',\n",
        "                '‚úÖ Record in quiet room',\n",
        "                '‚úÖ Reduce background noise',\n",
        "                '‚úÖ Maintain consistent volume'\n",
        "            ]\n",
        "        })\n",
        "    \n",
        "    return tips"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87Z7OdeA18uz",
      "metadata": {
        "id": "87Z7OdeA18uz"
      },
      "source": [
        "<b><h2> Fungsi Analisis Non Verbal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "6oqZyPswZ9qx",
      "metadata": {
        "id": "6oqZyPswZ9qx"
      },
      "outputs": [],
      "source": [
        "def extract_audio_fixed(video_path, audio_output_path=\"temp_audio.wav\"):\n",
        "    \"\"\"\n",
        "    Ekstrak audio menggunakan FFmpeg langsung untuk menghindari masalah MoviePy\n",
        "    dengan file WebM yang memiliki Duration: N/A\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"   ‚è≥ Mengekstrak audio dari {video_path}...\")\n",
        "\n",
        "        # Gunakan FFmpeg langsung via subprocess\n",
        "        command = [\n",
        "            'ffmpeg',\n",
        "            '-i', video_path,\n",
        "            '-vn',  # No video\n",
        "            '-acodec', 'pcm_s16le',  # Audio codec\n",
        "            '-ar', '44100',  # Sample rate\n",
        "            '-ac', '2',  # Audio channels\n",
        "            '-y',  # Overwrite output\n",
        "            audio_output_path\n",
        "        ]\n",
        "\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        os.makedirs(AUDIO_DIR, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(audio_output_path):\n",
        "            print(f\"   ‚úÖ Audio berhasil diekstrak: {audio_output_path}\")\n",
        "            return audio_output_path\n",
        "        else:\n",
        "            raise Exception(\"Audio extraction failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error ekstraksi audio: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "bct1jM4wZ_kF",
      "metadata": {
        "id": "bct1jM4wZ_kF"
      },
      "outputs": [],
      "source": [
        "def analyze_speech_tempo(audio_path):\n",
        "    # Load audio\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "    # Deteksi segmen non-silent (ketika berbicara)\n",
        "    nonsilent_ranges = detect_nonsilent(\n",
        "        audio,\n",
        "        min_silence_len=500,  # Jeda minimal 500ms dianggap pause\n",
        "        silence_thresh=-40     # Threshold volume untuk silence\n",
        "    )\n",
        "\n",
        "    # Hitung durasi total bicara\n",
        "    total_speaking_time = sum([(end - start) for start, end in nonsilent_ranges]) / 1000\n",
        "    total_duration = len(audio) / 1000\n",
        "\n",
        "    # Hitung jumlah jeda\n",
        "    num_pauses = len(nonsilent_ranges) - 1\n",
        "\n",
        "    # Hitung speech rate (kata per menit - estimasi)\n",
        "    # Asumsi: 1 detik bicara ‚âà 2-3 kata\n",
        "    estimated_words = total_speaking_time * 2.5\n",
        "    speech_rate = (estimated_words / total_speaking_time) * 60 if total_speaking_time > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"total_duration_seconds\": round(total_duration, 2),\n",
        "        \"speaking_time_seconds\": round(total_speaking_time, 2),\n",
        "        \"silence_time_seconds\": round(total_duration - total_speaking_time, 2),\n",
        "        \"number_of_pauses\": num_pauses,\n",
        "        \"speech_rate_wpm\": round(speech_rate, 2),\n",
        "        \"speaking_ratio\": round(total_speaking_time / total_duration, 2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "uh0y-vWyaBVm",
      "metadata": {
        "id": "uh0y-vWyaBVm"
      },
      "outputs": [],
      "source": [
        "def analyze_facial_expressions(video_path):\n",
        "    # Initialize MediaPipe Face Mesh\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "    face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    expression_data = {\n",
        "        \"smile_intensity\": [],\n",
        "        \"eyebrow_movement\": [],\n",
        "        \"head_pose\": []\n",
        "    }\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(rgb_frame)\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "            # Ekstrak landmark penting\n",
        "            # Mouth corners (senyum): 61, 291\n",
        "            # Lips: 13, 14\n",
        "            left_mouth = landmarks.landmark[61]\n",
        "            right_mouth = landmarks.landmark[291]\n",
        "            upper_lip = landmarks.landmark[13]\n",
        "            lower_lip = landmarks.landmark[14]\n",
        "\n",
        "            # Hitung intensitas senyum (jarak horizontal mouth corners)\n",
        "            smile_width = abs(right_mouth.x - left_mouth.x)\n",
        "            expression_data[\"smile_intensity\"].append(smile_width)\n",
        "\n",
        "            # Eyebrow position (landmark 70, 300 untuk alis)\n",
        "            left_eyebrow = landmarks.landmark[70]\n",
        "            right_eyebrow = landmarks.landmark[300]\n",
        "            eyebrow_height = (left_eyebrow.y + right_eyebrow.y) / 2\n",
        "            expression_data[\"eyebrow_movement\"].append(eyebrow_height)\n",
        "\n",
        "            # Head pose (estimasi dari nose tip: 1)\n",
        "            nose_tip = landmarks.landmark[1]\n",
        "            expression_data[\"head_pose\"].append({\n",
        "                \"x\": nose_tip.x,\n",
        "                \"y\": nose_tip.y,\n",
        "                \"z\": nose_tip.z\n",
        "            })\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Analisis statistik\n",
        "    return {\n",
        "        \"average_smile_intensity\": round(np.mean(expression_data[\"smile_intensity\"]), 4),\n",
        "        \"smile_variation\": round(np.std(expression_data[\"smile_intensity\"]), 4),\n",
        "        \"eyebrow_movement_range\": round(np.std(expression_data[\"eyebrow_movement\"]), 4),\n",
        "        \"total_frames_analyzed\": frame_count,\n",
        "        \"face_detected_percentage\": round(len(expression_data[\"smile_intensity\"]) / frame_count * 100, 2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "uNwvxyJKaDJZ",
      "metadata": {
        "id": "uNwvxyJKaDJZ"
      },
      "outputs": [],
      "source": [
        "def analyze_eye_movement(video_path):\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        refine_landmarks=True  # Penting untuk deteksi iris\n",
        "    )\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    eye_data = {\n",
        "        \"gaze_positions\": [],\n",
        "        \"blink_count\": 0,\n",
        "        \"eye_contact_percentage\": 0\n",
        "    }\n",
        "\n",
        "    prev_eye_closed = False\n",
        "    frame_count = 0\n",
        "    direct_gaze_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(rgb_frame)\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "            # Eye landmarks (mata kiri: 33, 133; mata kanan: 362, 263)\n",
        "            left_eye_top = landmarks.landmark[159]\n",
        "            left_eye_bottom = landmarks.landmark[145]\n",
        "            right_eye_top = landmarks.landmark[386]\n",
        "            right_eye_bottom = landmarks.landmark[374]\n",
        "\n",
        "            # Deteksi kedipan (Eye Aspect Ratio)\n",
        "            left_eye_height = abs(left_eye_top.y - left_eye_bottom.y)\n",
        "            right_eye_height = abs(right_eye_top.y - right_eye_bottom.y)\n",
        "            avg_eye_height = (left_eye_height + right_eye_height) / 2\n",
        "\n",
        "            # Threshold untuk mata tertutup\n",
        "            eye_closed = avg_eye_height < 0.01\n",
        "\n",
        "            if eye_closed and not prev_eye_closed:\n",
        "                eye_data[\"blink_count\"] += 1\n",
        "\n",
        "            prev_eye_closed = eye_closed\n",
        "\n",
        "            # Iris tracking untuk gaze direction\n",
        "            # Iris center landmarks: 468-473\n",
        "            if len(landmarks.landmark) > 473:\n",
        "                left_iris = landmarks.landmark[468]\n",
        "                right_iris = landmarks.landmark[473]\n",
        "\n",
        "                # Simpan posisi gaze\n",
        "                gaze_x = (left_iris.x + right_iris.x) / 2\n",
        "                gaze_y = (left_iris.y + right_iris.y) / 2\n",
        "                eye_data[\"gaze_positions\"].append({\"x\": gaze_x, \"y\": gaze_y})\n",
        "\n",
        "                # Deteksi eye contact (gaze ke tengah frame)\n",
        "                if 0.4 < gaze_x < 0.6 and 0.3 < gaze_y < 0.7:\n",
        "                    direct_gaze_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if frame_count > 0:\n",
        "        eye_data[\"eye_contact_percentage\"] = round((direct_gaze_count / frame_count) * 100, 2)\n",
        "        eye_data[\"blink_rate_per_minute\"] = round((eye_data[\"blink_count\"] / frame_count) * (30 * 60), 2)\n",
        "\n",
        "    return {\n",
        "        \"total_blinks\": eye_data[\"blink_count\"],\n",
        "        \"blink_rate_per_minute\": eye_data.get(\"blink_rate_per_minute\", 0),\n",
        "        \"eye_contact_percentage\": eye_data[\"eye_contact_percentage\"],\n",
        "        \"gaze_stability\": round(np.std([g[\"x\"] for g in eye_data[\"gaze_positions\"]]), 4) if eye_data[\"gaze_positions\"] else 0\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "mQQBBkQUaFEB",
      "metadata": {
        "id": "mQQBBkQUaFEB"
      },
      "outputs": [],
      "source": [
        "def interpret_non_verbal_analysis(analysis_json):\n",
        "    interpretations = {}\n",
        "\n",
        "    # --- Speech Analysis ---\n",
        "    speech = analysis_json.get(\"speech_analysis\", {})\n",
        "    if speech:\n",
        "        speaking_ratio = speech.get(\"speaking_ratio\", 0)\n",
        "        pauses = speech.get(\"number_of_pauses\", 0)\n",
        "        rate = speech.get(\"speech_rate_wpm\", 0)\n",
        "\n",
        "        if speaking_ratio > 0.6:\n",
        "            speech_summary = \"Kandidat cukup aktif berbicara.\"\n",
        "        else:\n",
        "            speech_summary = \"Kandidat lebih banyak diam dibanding berbicara.\"\n",
        "\n",
        "        if pauses > 30:\n",
        "            speech_summary += \" Namun sering berhenti sejenak, mungkin sedang berpikir.\"\n",
        "        else:\n",
        "            speech_summary += \" Jeda bicara relatif sedikit.\"\n",
        "\n",
        "        if 120 <= rate <= 160:\n",
        "            speech_summary += \" Kecepatan bicara normal.\"\n",
        "        elif rate > 160:\n",
        "            speech_summary += \" Bicara agak cepat.\"\n",
        "        else:\n",
        "            speech_summary += \" Bicara agak lambat.\"\n",
        "\n",
        "        interpretations[\"speech_analysis\"] = speech_summary\n",
        "\n",
        "    # --- Facial Expression Analysis ---\n",
        "    facial = analysis_json.get(\"facial_expression_analysis\", {})\n",
        "    if facial:\n",
        "        smile_intensity = facial.get(\"average_smile_intensity\", 0)\n",
        "        eyebrow_range = facial.get(\"eyebrow_movement_range\", 0)\n",
        "\n",
        "        if smile_intensity < 0.1:\n",
        "            facial_summary = \"Ekspresi wajah minim senyum, terlihat serius.\"\n",
        "        else:\n",
        "            facial_summary = \"Sering tersenyum, terlihat ramah.\"\n",
        "\n",
        "        if eyebrow_range < 0.01:\n",
        "            facial_summary += \" Gerakan alis minim, ekspresi emosional kurang.\"\n",
        "        else:\n",
        "            facial_summary += \" Gerakan alis cukup bervariasi.\"\n",
        "\n",
        "        interpretations[\"facial_expression_analysis\"] = facial_summary\n",
        "\n",
        "    # --- Eye Movement Analysis ---\n",
        "    eye = analysis_json.get(\"eye_movement_analysis\", {})\n",
        "    if eye:\n",
        "        blink_rate = eye.get(\"blink_rate_per_minute\", 0)\n",
        "        eye_contact = eye.get(\"eye_contact_percentage\", 0)\n",
        "\n",
        "        if eye_contact > 80:\n",
        "            eye_summary = \"Kontak mata sangat baik.\"\n",
        "        else:\n",
        "            eye_summary = \"Kontak mata kurang konsisten.\"\n",
        "\n",
        "        if blink_rate > 60:\n",
        "            eye_summary += \" Kedipan cukup sering, mungkin gugup.\"\n",
        "        else:\n",
        "            eye_summary += \" Kedipan normal.\"\n",
        "\n",
        "        interpretations[\"eye_movement_analysis\"] = eye_summary\n",
        "\n",
        "    return interpretations\n",
        "\n",
        "\n",
        "def analyze_interview_video(video_path, audio_path=None):\n",
        "    print(\"üé¨ Memulai analisis interview...\")\n",
        "\n",
        "    # Ekstrak audio jika belum ada\n",
        "    if audio_path is None:\n",
        "        print(\"üì§ Mengekstrak audio dari video...\")\n",
        "        filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        audio_path = os.path.join(AUDIO_DIR, f\"{filename}.wav\")\n",
        "        extract_audio_fixed(video_path, audio_path)\n",
        "\n",
        "    # Analisis non verbal\n",
        "    print(\"üé§ Analisis tempo bicara...\")\n",
        "    speech_analysis = analyze_speech_tempo(audio_path)\n",
        "\n",
        "    print(\"üòä Analisis ekspresi wajah...\")\n",
        "    facial_analysis = analyze_facial_expressions(video_path)\n",
        "\n",
        "    print(\"üëÅÔ∏è Analisis gerakan mata...\")\n",
        "    eye_analysis = analyze_eye_movement(video_path)\n",
        "\n",
        "    # Gabungan hasil\n",
        "    final_result = {\n",
        "        \"speech_analysis\": speech_analysis,\n",
        "        \"facial_expression_analysis\": facial_analysis,\n",
        "        \"eye_movement_analysis\": eye_analysis,\n",
        "    }\n",
        "\n",
        "    # Tambahkan interpretasi\n",
        "    final_result[\"interpretation\"] = interpret_non_verbal_analysis(final_result)\n",
        "\n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "VKY_L0zFlCj9",
      "metadata": {
        "id": "VKY_L0zFlCj9"
      },
      "outputs": [],
      "source": [
        "def analyze_videos_in_batch(video_paths):\n",
        "    \"\"\"\n",
        "    Melakukan analisis non-verbal untuk banyak video.\n",
        "    Mengembalikan list hasil (setiap item adalah hasil analisis 1 video).\n",
        "    \"\"\"\n",
        "\n",
        "    batch_results = []  # <--- LIST penyimpanan hasil\n",
        "\n",
        "    for video_path in video_paths:  # <--- LOOP semua video\n",
        "        print(f\"üîç Memproses video: {video_path}\")\n",
        "\n",
        "        result = analyze_interview_video(video_path)\n",
        "\n",
        "        batch_results.append({\n",
        "            \"video_path\": video_path,\n",
        "            \"result\": result\n",
        "        })\n",
        "\n",
        "    return batch_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "hU7cyKXApFrL",
      "metadata": {
        "id": "hU7cyKXApFrL"
      },
      "outputs": [],
      "source": [
        "def summarize_non_verbal_batch(assessment_results):\n",
        "    import numpy as np\n",
        "\n",
        "    speaking_ratios = []\n",
        "    pauses = []\n",
        "    speech_rates = []\n",
        "    smiles = []\n",
        "    eyebrows = []\n",
        "    eye_contacts = []\n",
        "    blink_rates = []\n",
        "    \n",
        "    # ‚úÖ NEW: Collect confidence scores\n",
        "    confidence_scores = []\n",
        "\n",
        "    for item in assessment_results:\n",
        "        nv = item[\"result\"][\"non_verbal_analysis\"]\n",
        "\n",
        "        sp = nv[\"speech_analysis\"]\n",
        "        speaking_ratios.append(sp[\"speaking_ratio\"])\n",
        "        pauses.append(sp[\"number_of_pauses\"])\n",
        "        speech_rates.append(sp[\"speech_rate_wpm\"])\n",
        "\n",
        "        fc = nv[\"facial_expression_analysis\"]\n",
        "        smiles.append(fc[\"average_smile_intensity\"])\n",
        "        eyebrows.append(fc[\"eyebrow_movement_range\"])\n",
        "\n",
        "        ey = nv[\"eye_movement_analysis\"]\n",
        "        eye_contacts.append(ey[\"eye_contact_percentage\"])\n",
        "        blink_rates.append(ey[\"blink_rate_per_minute\"])\n",
        "        \n",
        "        # ‚úÖ NEW: Collect confidence scores\n",
        "        conf_score = item[\"result\"].get(\"non_verbal_confidence_score\", 0)\n",
        "        confidence_scores.append(conf_score)\n",
        "\n",
        "    # ‚úÖ Calculate average confidence\n",
        "    avg_confidence = round(np.mean(confidence_scores), 2) if confidence_scores else 0\n",
        "    \n",
        "    # ‚úÖ Determine confidence level\n",
        "    if avg_confidence >= 85:\n",
        "        confidence_level = \"Very High\"\n",
        "    elif avg_confidence >= 75:\n",
        "        confidence_level = \"High\"\n",
        "    elif avg_confidence >= 60:\n",
        "        confidence_level = \"Medium\"\n",
        "    elif avg_confidence >= 45:\n",
        "        confidence_level = \"Low\"\n",
        "    else:\n",
        "        confidence_level = \"Very Low\"\n",
        "\n",
        "    aggregated = {\n",
        "        \"speech_analysis\": {\n",
        "            \"avg_speaking_ratio\": round(np.mean(speaking_ratios), 3),\n",
        "            \"avg_pauses\": round(np.mean(pauses), 2),\n",
        "            \"avg_speech_rate\": round(np.mean(speech_rates), 2)\n",
        "        },\n",
        "        \"facial_expression_analysis\": {\n",
        "            \"avg_smile_intensity\": round(np.mean(smiles), 4),\n",
        "            \"avg_eyebrow_movement_range\": round(np.mean(eyebrows), 4)\n",
        "        },\n",
        "        \"eye_movement_analysis\": {\n",
        "            \"avg_eye_contact\": round(np.mean(eye_contacts), 2),\n",
        "            \"avg_blink_rate\": round(np.mean(blink_rates), 2)\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Build summary text (existing code)\n",
        "    summary_parts = []\n",
        "\n",
        "    ratio = aggregated[\"speech_analysis\"][\"avg_speaking_ratio\"]\n",
        "    pauses_avg = aggregated[\"speech_analysis\"][\"avg_pauses\"]\n",
        "    speed = aggregated[\"speech_analysis\"][\"avg_speech_rate\"]\n",
        "\n",
        "    if ratio > 0.6:\n",
        "        summary_parts.append(\"Kandidat cukup aktif berbicara\")\n",
        "    else:\n",
        "        summary_parts.append(\"Kandidat cenderung pasif dalam berbicara\")\n",
        "\n",
        "    if pauses_avg > 30:\n",
        "        summary_parts.append(\"dengan jeda bicara yang sering\")\n",
        "    else:\n",
        "        summary_parts.append(\"dengan jeda bicara yang jarang\")\n",
        "\n",
        "    if speed > 160:\n",
        "        summary_parts.append(\"dan berbicara dengan kecepatan cukup cepat.\")\n",
        "    elif speed < 120:\n",
        "        summary_parts.append(\"dan berbicara dengan kecepatan cenderung lambat.\")\n",
        "    else:\n",
        "        summary_parts.append(\"dan kecepatan bicara normal.\")\n",
        "\n",
        "    smile = aggregated[\"facial_expression_analysis\"][\"avg_smile_intensity\"]\n",
        "    eyebrow = aggregated[\"facial_expression_analysis\"][\"avg_eyebrow_movement_range\"]\n",
        "\n",
        "    if smile < 0.1:\n",
        "        summary_parts.append(\"Ekspresi wajah terlihat serius dan minim senyum.\")\n",
        "    else:\n",
        "        summary_parts.append(\"Ekspresi wajah cukup positif dan sering tersenyum.\")\n",
        "\n",
        "    if eyebrow < 0.01:\n",
        "        summary_parts.append(\"Gerakan alis minim, menunjukkan ekspresi emosional yang rendah.\")\n",
        "    else:\n",
        "        summary_parts.append(\"Gerakan alis cukup variatif.\")\n",
        "\n",
        "    eye_contact = aggregated[\"eye_movement_analysis\"][\"avg_eye_contact\"]\n",
        "    blink = aggregated[\"eye_movement_analysis\"][\"avg_blink_rate\"]\n",
        "\n",
        "    if eye_contact > 80:\n",
        "        summary_parts.append(\"Kontak mata sangat baik.\")\n",
        "    else:\n",
        "        summary_parts.append(\"Kontak mata kurang konsisten.\")\n",
        "\n",
        "    if blink > 60:\n",
        "        summary_parts.append(\"Tingkat kedipan cukup tinggi yang dapat menandakan ketegangan.\")\n",
        "    else:\n",
        "        summary_parts.append(\"Tingkat kedipan normal.\")\n",
        "\n",
        "    summary_text = \" \".join(summary_parts)\n",
        "    aggregated[\"summary\"] = summary_text\n",
        "\n",
        "    # ‚úÖ NEW: Return with confidence scores\n",
        "    return {\n",
        "        \"total_videos\": len(assessment_results),\n",
        "        \"aggregated_non_verbal\": aggregated,\n",
        "        \"average_confidence_score\": avg_confidence,  # ‚úÖ ADDED\n",
        "        \"overall_confidence_level\": confidence_level  # ‚úÖ ADDED\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94107775",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "6f9bbb9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_interview_video_with_confidence(video_path, audio_path=None):\n",
        "    \"\"\"\n",
        "    Analyze interview video with confidence scoring\n",
        "    Returns analysis + confidence score (0-100)\n",
        "    \"\"\"\n",
        "    print(\"üé¨ Memulai analisis interview...\")\n",
        "\n",
        "    # Ekstrak audio jika belum ada\n",
        "    if audio_path is None:\n",
        "        print(\"üì§ Mengekstrak audio dari video...\")\n",
        "        filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        audio_path = os.path.join(AUDIO_DIR, f\"{filename}.wav\")\n",
        "        audio_extracted = extract_audio_fixed(video_path, audio_path)\n",
        "        \n",
        "        if not audio_extracted:\n",
        "            return {\n",
        "                'analysis': {\n",
        "                    'speech_analysis': {},\n",
        "                    'facial_expression_analysis': {},\n",
        "                    'eye_movement_analysis': {},\n",
        "                    'interpretation': {}\n",
        "                },\n",
        "                'confidence_score': 0,\n",
        "                'confidence_level': 'Failed',\n",
        "                'confidence_components': {}\n",
        "            }\n",
        "\n",
        "    # Analisis non verbal\n",
        "    print(\"üé§ Analisis tempo bicara...\")\n",
        "    speech_analysis = analyze_speech_tempo(audio_path)\n",
        "\n",
        "    print(\"üòä Analisis ekspresi wajah...\")\n",
        "    facial_analysis = analyze_facial_expressions(video_path)\n",
        "\n",
        "    print(\"üëÅÔ∏è Analisis gerakan mata...\")\n",
        "    eye_analysis = analyze_eye_movement(video_path)\n",
        "\n",
        "    # Gabungan hasil\n",
        "    analysis_result = {\n",
        "        \"speech_analysis\": speech_analysis,\n",
        "        \"facial_expression_analysis\": facial_analysis,\n",
        "        \"eye_movement_analysis\": eye_analysis,\n",
        "    }\n",
        "\n",
        "    # Tambahkan interpretasi\n",
        "    analysis_result[\"interpretation\"] = interpret_non_verbal_analysis(analysis_result)\n",
        "\n",
        "    # ‚úÖ CALCULATE CONFIDENCE SCORE\n",
        "    confidence_components = {}\n",
        "\n",
        "    # 1Ô∏è‚É£ Speech Analysis Confidence\n",
        "    speaking_ratio = speech_analysis.get('speaking_ratio', 0)\n",
        "    speech_rate = speech_analysis.get('speech_rate_wpm', 0)\n",
        "    \n",
        "    if speaking_ratio > 0.6 and 100 < speech_rate < 180:\n",
        "        speech_conf = 95\n",
        "    elif speaking_ratio > 0.4 and 80 < speech_rate < 200:\n",
        "        speech_conf = 80\n",
        "    elif speaking_ratio > 0.2:\n",
        "        speech_conf = 60\n",
        "    else:\n",
        "        speech_conf = 40\n",
        "    \n",
        "    confidence_components['speech_confidence'] = speech_conf\n",
        "\n",
        "    # 2Ô∏è‚É£ Facial Analysis Confidence\n",
        "    face_detected_pct = facial_analysis.get('face_detected_percentage', 0)\n",
        "    frames_analyzed = facial_analysis.get('total_frames_analyzed', 0)\n",
        "    \n",
        "    if face_detected_pct > 80 and frames_analyzed > 100:\n",
        "        facial_conf = 95\n",
        "    elif face_detected_pct > 60 and frames_analyzed > 50:\n",
        "        facial_conf = 80\n",
        "    elif face_detected_pct > 40 and frames_analyzed > 20:\n",
        "        facial_conf = 65\n",
        "    else:\n",
        "        facial_conf = 50\n",
        "    \n",
        "    confidence_components['facial_confidence'] = facial_conf\n",
        "\n",
        "    # 3Ô∏è‚É£ Eye Movement Confidence\n",
        "    eye_contact_pct = eye_analysis.get('eye_contact_percentage', 0)\n",
        "    blink_rate = eye_analysis.get('blink_rate_per_minute', 0)\n",
        "    \n",
        "    if eye_contact_pct > 50 and 15 < blink_rate < 40:\n",
        "        eye_conf = 90\n",
        "    elif eye_contact_pct > 30 and 10 < blink_rate < 50:\n",
        "        eye_conf = 75\n",
        "    elif eye_contact_pct > 10:\n",
        "        eye_conf = 60\n",
        "    else:\n",
        "        eye_conf = 45\n",
        "    \n",
        "    confidence_components['eye_confidence'] = eye_conf\n",
        "\n",
        "    # 4Ô∏è‚É£ Overall Data Quality\n",
        "    total_duration = speech_analysis.get('total_duration_seconds', 0)\n",
        "    \n",
        "    if total_duration > 30:\n",
        "        duration_conf = 100\n",
        "    elif total_duration > 15:\n",
        "        duration_conf = 85\n",
        "    elif total_duration > 5:\n",
        "        duration_conf = 70\n",
        "    else:\n",
        "        duration_conf = 50\n",
        "    \n",
        "    confidence_components['duration_confidence'] = duration_conf\n",
        "\n",
        "    # ‚úÖ CALCULATE OVERALL CONFIDENCE\n",
        "    overall_confidence = int(\n",
        "        (speech_conf * 0.3) +\n",
        "        (facial_conf * 0.3) +\n",
        "        (eye_conf * 0.25) +\n",
        "        (duration_conf * 0.15)\n",
        "    )\n",
        "\n",
        "    # Determine confidence level\n",
        "    if overall_confidence >= 85:\n",
        "        confidence_level = \"Very High\"\n",
        "    elif overall_confidence >= 75:\n",
        "        confidence_level = \"High\"\n",
        "    elif overall_confidence >= 60:\n",
        "        confidence_level = \"Medium\"\n",
        "    elif overall_confidence >= 45:\n",
        "        confidence_level = \"Low\"\n",
        "    else:\n",
        "        confidence_level = \"Very Low\"\n",
        "\n",
        "    print(f'\\n‚úÖ Non-Verbal Analysis Complete')\n",
        "    print(f'   Confidence: {overall_confidence}% ({confidence_level})')\n",
        "    print(f'   Components: Speech={speech_conf}%, Face={facial_conf}%, Eye={eye_conf}%, Duration={duration_conf}%\\n')\n",
        "\n",
        "    return {\n",
        "        'analysis': analysis_result,\n",
        "        'confidence_score': overall_confidence,\n",
        "        'confidence_level': confidence_level,\n",
        "        'confidence_components': confidence_components\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBNP5gUF8AN-",
      "metadata": {
        "id": "iBNP5gUF8AN-"
      },
      "source": [
        "<b><h2> Pengecekan model analisis non verbal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "VMhTrS4J7cfj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMhTrS4J7cfj",
        "outputId": "24e71b1b-3916-4900-a5c6-650d62c3e3ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        }
      ],
      "source": [
        "!find / -type f -name \"*.tflite\" 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AnWc6WJrKXFj",
      "metadata": {
        "id": "AnWc6WJrKXFj"
      },
      "source": [
        "<b><h2> Fungsi Transkrip Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "n_171HaCULti",
      "metadata": {
        "id": "n_171HaCULti"
      },
      "outputs": [],
      "source": [
        "def clean_repetitive_text(text, max_repetitions=3):\n",
        "    \"\"\"Remove repetitive patterns at the end of transcription\"\"\"\n",
        "    # Remove excessive repetitions (more than max_repetitions)\n",
        "    words = text.split()\n",
        "    if len(words) < 10:\n",
        "        return text\n",
        "\n",
        "    # Check last 100 words for repetitions\n",
        "    check_window = min(100, len(words))\n",
        "    last_words = words[-check_window:]\n",
        "\n",
        "    # Detect if last word repeats excessively\n",
        "    if len(last_words) > max_repetitions:\n",
        "        last_word = last_words[-1]\n",
        "\n",
        "        # Count consecutive repetitions from the end\n",
        "        repetition_count = 0\n",
        "        for word in reversed(last_words):\n",
        "            if word.lower() == last_word.lower():\n",
        "                repetition_count += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # If repetition exceeds threshold, remove them\n",
        "        if repetition_count > max_repetitions:\n",
        "            # Keep only max_repetitions of the repeated word\n",
        "            words = words[:-repetition_count] + [last_word] * max_repetitions\n",
        "            print(f'   üßπ Cleaned {repetition_count - max_repetitions} repetitive words')\n",
        "\n",
        "    # Remove common hallucination patterns\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    # Pattern: word repeated 5+ times in a row\n",
        "    cleaned_text = re.sub(r'\\b(\\w+)(?:\\s+\\1){4,}\\b', r'\\1', cleaned_text)\n",
        "\n",
        "    return cleaned_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "dd4wVEyHUPYy",
      "metadata": {
        "id": "dd4wVEyHUPYy"
      },
      "outputs": [],
      "source": [
        "def transcribe_video(video_path):\n",
        "    \"\"\"Transcribe video using faster-whisper with MAXIMUM ACCURACY settings\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(video_path):\n",
        "            raise Exception(f\"Video file not found: {video_path}\")\n",
        "\n",
        "        if not os.access(video_path, os.R_OK):\n",
        "            raise Exception(f\"Video file is not readable: {video_path}\")\n",
        "\n",
        "        file_size = os.path.getsize(video_path) / (1024 * 1024)\n",
        "        print(f'üìÅ Video: {os.path.basename(video_path)} ({file_size:.2f} MB)')\n",
        "\n",
        "        print('üîÑ Starting transcription...')\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Dynamic parameters based on file size\n",
        "        if file_size > 30:\n",
        "            print('   ‚ö° Large file - using balanced mode')\n",
        "            beam_size = 3\n",
        "            best_of = 3\n",
        "        else:\n",
        "            beam_size = 5\n",
        "            best_of = 5\n",
        "\n",
        "        # Transcribe with improved hallucination prevention\n",
        "        segments, info = whisper_model.transcribe(\n",
        "            video_path,\n",
        "            language=\"en\",\n",
        "            task=\"transcribe\",\n",
        "            beam_size=beam_size,\n",
        "            best_of=best_of,\n",
        "            patience=2.0,\n",
        "            length_penalty=1.0,\n",
        "            repetition_penalty=1.2,  # INCREASED from 1.0 to 1.2\n",
        "            temperature=0.0,\n",
        "            compression_ratio_threshold=2.4,\n",
        "            log_prob_threshold=-1.0,\n",
        "            no_speech_threshold=0.6,\n",
        "            condition_on_previous_text=False,  # CHANGED to False to prevent repetition\n",
        "            initial_prompt=\"This is a professional interview conversation in clear English. The speaker is answering interview questions.\",\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(\n",
        "                threshold=0.5,\n",
        "                min_speech_duration_ms=250,\n",
        "                max_speech_duration_s=float('inf'),\n",
        "                min_silence_duration_ms=2000,\n",
        "                speech_pad_ms=400\n",
        "            ),\n",
        "            word_timestamps=False,\n",
        "            hallucination_silence_threshold=2.0  # CHANGED from None to 2.0\n",
        "        )\n",
        "\n",
        "        # Collect segments with progress bar\n",
        "        print('   üìù Collecting segments...')\n",
        "        transcription_text = \"\"\n",
        "        segments_list = list(segments)\n",
        "\n",
        "        for segment in tqdm(segments_list, desc=\"   Segments\", unit=\"seg\", ncols=80, leave=False):\n",
        "            transcription_text += segment.text + \" \"\n",
        "\n",
        "        transcription_text = transcription_text.strip()\n",
        "\n",
        "        if not transcription_text:\n",
        "            print('   ‚ö†Ô∏è  No speech detected')\n",
        "            return \"[No speech detected in video]\"\n",
        "\n",
        "        # CLEAN REPETITIVE TEXT\n",
        "        original_length = len(transcription_text)\n",
        "        transcription_text = clean_repetitive_text(transcription_text, max_repetitions=3)\n",
        "\n",
        "        if len(transcription_text) < original_length:\n",
        "            print(f'   üßπ Cleaned: {original_length} ‚Üí {len(transcription_text)} chars')\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        words = transcription_text.split()\n",
        "\n",
        "        print(f'   ‚úÖ Completed in {total_time:.1f}s | {len(segments_list)} segments | {len(words)} words')\n",
        "\n",
        "        # Cleanup\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        return transcription_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ùå Error: {str(e)}')\n",
        "        gc.collect()\n",
        "        raise Exception(f\"Transcription failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CeRNTro8KcBf",
      "metadata": {
        "id": "CeRNTro8KcBf"
      },
      "source": [
        "<b><h2> Fungsi Translate to Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "BFNzaF6FUUVM",
      "metadata": {
        "id": "BFNzaF6FUUVM"
      },
      "outputs": [],
      "source": [
        "def translate_to_indonesian(text):\n",
        "    \"\"\"Translate English text to Indonesian using DeepL\"\"\"\n",
        "    if not translator:\n",
        "        print('   ‚ö†Ô∏è  Translation skipped (no API key)')\n",
        "        return \"[Translation not available]\"\n",
        "\n",
        "    try:\n",
        "        max_chunk_size = 5000\n",
        "\n",
        "        if len(text) <= max_chunk_size:\n",
        "            result = translator.translate_text(text, source_lang=\"EN\", target_lang=\"ID\")\n",
        "            translated_text = result.text\n",
        "        else:\n",
        "            sentences = text.split('. ')\n",
        "            translated_sentences = []\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            # Progress bar for translation chunks\n",
        "            for sentence in tqdm(sentences, desc=\"   Translation\", unit=\"sent\", ncols=80, leave=False):\n",
        "                if len(current_chunk) + len(sentence) < max_chunk_size:\n",
        "                    current_chunk += sentence + \". \"\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        result = translator.translate_text(current_chunk.strip(), source_lang=\"EN\", target_lang=\"ID\")\n",
        "                        translated_sentences.append(result.text)\n",
        "                    current_chunk = sentence + \". \"\n",
        "\n",
        "            if current_chunk:\n",
        "                result = translator.translate_text(current_chunk.strip(), source_lang=\"EN\", target_lang=\"ID\")\n",
        "                translated_sentences.append(result.text)\n",
        "\n",
        "            translated_text = \" \".join(translated_sentences)\n",
        "\n",
        "        print(f'   ‚úÖ Translation: {len(text)} ‚Üí {len(translated_text)} chars')\n",
        "        return translated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ùå Translation failed: {str(e)}')\n",
        "        return f\"[Translation failed: {str(e)}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "fb9ff04d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_to_indonesian_with_confidence(text):\n",
        "    \"\"\"Translate English text to Indonesian using DeepL with confidence scoring\"\"\"\n",
        "    if not translator:\n",
        "        print('   ‚ö†Ô∏è  Translation skipped (no API key)')\n",
        "        return {\n",
        "            'translated_text': \"[Translation not available]\",\n",
        "            'confidence_score': 0,\n",
        "            'confidence_level': 'N/A',\n",
        "            'quality_metrics': {}\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        max_chunk_size = 5000\n",
        "        translation_start = time.time()\n",
        "\n",
        "        if len(text) <= max_chunk_size:\n",
        "            result = translator.translate_text(text, source_lang=\"EN\", target_lang=\"ID\")\n",
        "            translated_text = result.text\n",
        "            chunks_processed = 1\n",
        "        else:\n",
        "            sentences = text.split('. ')\n",
        "            translated_sentences = []\n",
        "            current_chunk = \"\"\n",
        "            chunks_processed = 0\n",
        "\n",
        "            for sentence in tqdm(sentences, desc=\"   Translation\", unit=\"sent\", ncols=80, leave=False):\n",
        "                if len(current_chunk) + len(sentence) < max_chunk_size:\n",
        "                    current_chunk += sentence + \". \"\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        result = translator.translate_text(current_chunk.strip(), source_lang=\"EN\", target_lang=\"ID\")\n",
        "                        translated_sentences.append(result.text)\n",
        "                        chunks_processed += 1\n",
        "                    current_chunk = sentence + \". \"\n",
        "\n",
        "            if current_chunk:\n",
        "                result = translator.translate_text(current_chunk.strip(), source_lang=\"EN\", target_lang=\"ID\")\n",
        "                translated_sentences.append(result.text)\n",
        "                chunks_processed += 1\n",
        "\n",
        "            translated_text = \" \".join(translated_sentences)\n",
        "\n",
        "        translation_time = time.time() - translation_start\n",
        "\n",
        "        # ‚úÖ Calculate confidence score based on:\n",
        "        # 1. Length similarity (source vs target)\n",
        "        # 2. Processing time (faster = more confident API response)\n",
        "        # 3. Character count coverage\n",
        "        \n",
        "        source_len = len(text)\n",
        "        target_len = len(translated_text)\n",
        "        \n",
        "        # Length ratio (ideal: 0.8-1.2 for EN‚ÜíID)\n",
        "        length_ratio = target_len / source_len if source_len > 0 else 0\n",
        "        if 0.8 <= length_ratio <= 1.2:\n",
        "            length_confidence = 100\n",
        "        elif 0.6 <= length_ratio <= 1.4:\n",
        "            length_confidence = 80\n",
        "        elif 0.4 <= length_ratio <= 1.6:\n",
        "            length_confidence = 60\n",
        "        else:\n",
        "            length_confidence = 40\n",
        "\n",
        "        # Processing speed confidence\n",
        "        chars_per_second = source_len / translation_time if translation_time > 0 else 0\n",
        "        if chars_per_second > 1000:\n",
        "            speed_confidence = 100\n",
        "        elif chars_per_second > 500:\n",
        "            speed_confidence = 90\n",
        "        elif chars_per_second > 200:\n",
        "            speed_confidence = 80\n",
        "        else:\n",
        "            speed_confidence = 70\n",
        "\n",
        "        # API reliability (based on successful chunks)\n",
        "        if chunks_processed == 1:\n",
        "            api_confidence = 100  # Single chunk = direct API call\n",
        "        else:\n",
        "            api_confidence = 95  # Multiple chunks still reliable\n",
        "\n",
        "        # Coverage (how much of source was translated)\n",
        "        if target_len > 0:\n",
        "            coverage_confidence = min(100, (target_len / source_len) * 100)\n",
        "        else:\n",
        "            coverage_confidence = 0\n",
        "\n",
        "        # ‚úÖ Weighted average\n",
        "        overall_confidence = int(\n",
        "            (length_confidence * 0.3) +\n",
        "            (speed_confidence * 0.2) +\n",
        "            (api_confidence * 0.3) +\n",
        "            (coverage_confidence * 0.2)\n",
        "        )\n",
        "\n",
        "        # Determine confidence level\n",
        "        if overall_confidence >= 90:\n",
        "            confidence_level = \"Very High\"\n",
        "        elif overall_confidence >= 80:\n",
        "            confidence_level = \"High\"\n",
        "        elif overall_confidence >= 70:\n",
        "            confidence_level = \"Medium\"\n",
        "        elif overall_confidence >= 50:\n",
        "            confidence_level = \"Low\"\n",
        "        else:\n",
        "            confidence_level = \"Very Low\"\n",
        "\n",
        "        print(f'   ‚úÖ Translation: {source_len} ‚Üí {target_len} chars')\n",
        "        print(f'   üìä Confidence: {overall_confidence}% ({confidence_level})')\n",
        "        print(f'      Length: {length_confidence}% | Speed: {speed_confidence}% | API: {api_confidence}% | Coverage: {coverage_confidence:.0f}%')\n",
        "\n",
        "        return {\n",
        "            'translated_text': translated_text,\n",
        "            'confidence_score': overall_confidence,\n",
        "            'confidence_level': confidence_level,\n",
        "            'quality_metrics': {\n",
        "                'length_confidence': length_confidence,\n",
        "                'speed_confidence': speed_confidence,\n",
        "                'api_confidence': api_confidence,\n",
        "                'coverage_confidence': int(coverage_confidence),\n",
        "                'length_ratio': round(length_ratio, 2),\n",
        "                'chars_per_second': int(chars_per_second),\n",
        "                'chunks_processed': chunks_processed,\n",
        "                'translation_time': round(translation_time, 2)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'   ‚ùå Translation failed: {str(e)}')\n",
        "        return {\n",
        "            'translated_text': f\"[Translation failed: {str(e)}]\",\n",
        "            'confidence_score': 0,\n",
        "            'confidence_level': 'Failed',\n",
        "            'quality_metrics': {'error': str(e)}\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DI6e_7X9Kgyp",
      "metadata": {
        "id": "DI6e_7X9Kgyp"
      },
      "source": [
        "<b><h2> Fungsi Pembuatan Dummy data ( sementara )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "Gcog2Af9UVwA",
      "metadata": {
        "id": "Gcog2Af9UVwA"
      },
      "outputs": [],
      "source": [
        "def generate_dummy_assessment(transcription_text, position_id, transcription_id=None, question=\"\"):\n",
        "    \"\"\"Generate dummy assessment data untuk testing - DEPRECATED, use LLM evaluation instead\"\"\"\n",
        "    words = transcription_text.split()\n",
        "    word_count = len(words)\n",
        "    char_count = len(transcription_text)\n",
        "\n",
        "    confidence_score = random.randint(85, 98)\n",
        "    kualitas_jawaban = random.randint(80, 100)\n",
        "    relevansi = random.randint(75, 95)\n",
        "    koherensi = random.randint(70, 90)\n",
        "    tempo_bicara = random.randint(80, 100)\n",
        "\n",
        "    total = round((confidence_score + kualitas_jawaban + relevansi + koherensi + tempo_bicara) / 5)\n",
        "\n",
        "    if total >= 90:\n",
        "        penilaian_akhir = 5\n",
        "    elif total >= 80:\n",
        "        penilaian_akhir = 4\n",
        "    elif total >= 70:\n",
        "        penilaian_akhir = 3\n",
        "    elif total >= 60:\n",
        "        penilaian_akhir = 2\n",
        "    else:\n",
        "        penilaian_akhir = 1\n",
        "\n",
        "    has_cheating = random.choice([True, False, False, False])\n",
        "\n",
        "    if has_cheating:\n",
        "        cheating_detection = \"Ya\"\n",
        "        alasan_cheating = random.choice([\n",
        "            \"Terdeteksi adanya manipulasi suara\",\n",
        "            \"Terdeteksi multiple speakers\",\n",
        "            \"Pola jawaban tidak konsisten\",\n",
        "            \"Kecepatan bicara tidak natural\"\n",
        "        ])\n",
        "    else:\n",
        "        cheating_detection = \"Tidak\"\n",
        "        alasan_cheating = \"Tidak ada indikasi kecurangan\"\n",
        "\n",
        "    analisis_options = [\n",
        "        \"Lancar dan tidak mencurigakan\",\n",
        "        \"Sedikit gugup namun natural\",\n",
        "        \"Sangat percaya diri\",\n",
        "        \"Tempo bicara konsisten\",\n",
        "        \"Artikulasi jelas\"\n",
        "    ]\n",
        "    analisis_non_verbal = random.choice(analisis_options)\n",
        "\n",
        "    if penilaian_akhir >= 4 and not has_cheating:\n",
        "        keputusan_akhir = \"Lulus\"\n",
        "    elif penilaian_akhir >= 3 and not has_cheating:\n",
        "        keputusan_akhir = \"Pertimbangan\"\n",
        "    else:\n",
        "        keputusan_akhir = \"Tidak Lulus\"\n",
        "\n",
        "    return {\n",
        "        \"penilaian\": {\n",
        "            \"confidence_score\": confidence_score,\n",
        "            \"kualitas_jawaban\": kualitas_jawaban,\n",
        "            \"relevansi\": relevansi,\n",
        "            \"koherensi\": koherensi,\n",
        "            \"tempo_bicara\": tempo_bicara,\n",
        "            \"total\": total\n",
        "        },\n",
        "        \"penilaian_akhir\": penilaian_akhir,\n",
        "        \"cheating_detection\": cheating_detection,\n",
        "        \"alasan_cheating\": alasan_cheating,\n",
        "        \"analisis_non_verbal\": analisis_non_verbal,\n",
        "        \"keputusan_akhir\": keputusan_akhir,\n",
        "        \"transkripsi_en\": transcription_text,\n",
        "        \"transkripsi_id\": transcription_id,\n",
        "        \"metadata\": {\n",
        "            \"word_count\": word_count,\n",
        "            \"char_count\": char_count,\n",
        "            \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"translation_available\": transcription_id is not None  # NEW\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aUwU-2YTKlkS",
      "metadata": {
        "id": "aUwU-2YTKlkS"
      },
      "source": [
        "<b><h2> Initialize HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N38Dkix6w4p3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N38Dkix6w4p3",
        "outputId": "c35ac1f1-b39a-42d6-8cd2-5821646f9465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Initializing HuggingFace Inference API...\n",
            "‚ÑπÔ∏è  Using meta-llama/Llama-3.1-8B-Instruct via Inference API\n",
            "   No model download required - uses cloud API\n",
            "‚úÖ Inference API initialized successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ HuggingFace API Token\n",
        "HF_TOKEN = \"TOKENTOKENTOKEN\"  # Replace with your actual token\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "\n",
        "# Initialize Inference Client\n",
        "print('üì• Initializing HuggingFace Inference API...')\n",
        "print('‚ÑπÔ∏è  Using meta-llama/Llama-3.1-8B-Instruct via Inference API')\n",
        "print('   No model download required - uses cloud API')\n",
        "\n",
        "client = InferenceClient(api_key=HF_TOKEN)\n",
        "\n",
        "print('‚úÖ Inference API initialized successfully\\n')\n",
        "\n",
        "def evaluate_with_llm(transcription_text: str, question: str, position_id: int):\n",
        "    \"\"\"Evaluate interview answer using Llama-3.1-8B-Instruct via Inference API\"\"\"\n",
        "    try:\n",
        "        # Construct evaluation prompt\n",
        "        user_message = f\"\"\"You are an expert interview evaluator. Analyze the candidate's answer objectively and provide scores.\n",
        "\n",
        "Question: \"{question}\"\n",
        "\n",
        "Candidate's Answer: \"{transcription_text}\"\n",
        "\n",
        "Evaluate the answer on these 3 criteria (score 1-100 for each):\n",
        "1. Quality of answer (clarity, completeness, depth of knowledge)\n",
        "2. Coherence (logical flow, consistency, structure)\n",
        "3. Relevance (alignment with the question, staying on topic)\n",
        "\n",
        "Return ONLY valid JSON in this exact format:\n",
        "{{\n",
        "  \"kualitas_jawaban\": <score 1-100>,\n",
        "  \"koherensi\": <score 1-100>,\n",
        "  \"relevansi\": <score 1-100>,\n",
        "  \"analysis\": \"<brief explanation of the 3 scores>\"\n",
        "}}\"\"\"\n",
        "\n",
        "        print(f'‚îÇ ü§ñ Llama-3.1 Inference API Evaluation (3 criteria)...')\n",
        "\n",
        "        # Call Inference API\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an expert interview evaluator. Always respond with valid JSON only.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_message\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "        # Extract response\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        print(f'‚îÇ üì® API Response received ({len(response)} chars)')\n",
        "\n",
        "        # Extract JSON from response\n",
        "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            json_str = json_match.group(0)\n",
        "            evaluation = json_module.loads(json_str)\n",
        "        else:\n",
        "            raise ValueError(\"No valid JSON found in API response\")\n",
        "\n",
        "        # Validate LLM scores (only 3 criteria)\n",
        "        required_keys = ['kualitas_jawaban', 'koherensi', 'relevansi']\n",
        "        for key in required_keys:\n",
        "            if key not in evaluation:\n",
        "                raise ValueError(f\"Missing required key: {key}\")\n",
        "            # Ensure scores are in valid range\n",
        "            evaluation[key] = max(1, min(100, int(evaluation[key])))\n",
        "\n",
        "        # STATIC DUMMY VALUES for tempo_bicara and confidence_score\n",
        "        evaluation['tempo_bicara'] = 85\n",
        "        evaluation['confidence_score'] = 82\n",
        "\n",
        "        print(f'‚îÇ üìä LLM Scores: Quality={evaluation[\"kualitas_jawaban\"]}, Coherence={evaluation[\"koherensi\"]}, Relevance={evaluation[\"relevansi\"]}')\n",
        "        print(f'‚îÇ üìå Static: Tempo={evaluation[\"tempo_bicara\"]}, Confidence={evaluation[\"confidence_score\"]}')\n",
        "\n",
        "        # Calculate total from all 5 scores\n",
        "        total = round((\n",
        "            evaluation['confidence_score'] +\n",
        "            evaluation['kualitas_jawaban'] +\n",
        "            evaluation['relevansi'] +\n",
        "            evaluation['koherensi'] +\n",
        "            evaluation['tempo_bicara']\n",
        "        ) / 5)\n",
        "\n",
        "        if total >= 90:\n",
        "            penilaian_akhir = 5\n",
        "        elif total >= 80:\n",
        "            penilaian_akhir = 4\n",
        "        elif total >= 70:\n",
        "            penilaian_akhir = 3\n",
        "        elif total >= 60:\n",
        "            penilaian_akhir = 2\n",
        "        else:\n",
        "            penilaian_akhir = 1\n",
        "\n",
        "        cheating_detected = False\n",
        "        cheating_reason = \"Tidak ada indikasi kecurangan\"\n",
        "        if penilaian_akhir >= 4 and not cheating_detected:\n",
        "            keputusan_akhir = \"Lulus\"\n",
        "        elif penilaian_akhir >= 3 and not cheating_detected:\n",
        "            keputusan_akhir = \"Pertimbangan\"\n",
        "        else:\n",
        "            keputusan_akhir = \"Tidak Lulus\"\n",
        "\n",
        "        print(f'‚îÇ ‚úÖ Total Score: {total}/100 | Rating: {penilaian_akhir}/5 | Decision: {keputusan_akhir}')\n",
        "\n",
        "        return {\n",
        "            \"scores\": evaluation,\n",
        "            \"total\": total,\n",
        "            \"penilaian_akhir\": penilaian_akhir,\n",
        "            \"cheating_detected\": \"Ya\" if cheating_detected else \"Tidak\",\n",
        "            \"cheating_reason\": cheating_reason,\n",
        "            \"analysis\": evaluation.get('analysis', 'No analysis provided'),\n",
        "            \"keputusan_akhir\": keputusan_akhir,\n",
        "            \"scoring_method\": {\n",
        "                \"llm_evaluated\": [\"kualitas_jawaban\", \"koherensi\", \"relevansi\"],\n",
        "                \"static_dummy\": [\"tempo_bicara\", \"confidence_score\"]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'‚îÇ ‚ö†Ô∏è  Inference API evaluation failed: {str(e)}')\n",
        "        print(f'‚îÇ üîÑ Falling back to rule-based assessment...')\n",
        "\n",
        "        # Fallback\n",
        "        word_count = len(transcription_text.split())\n",
        "\n",
        "        if word_count < 10:\n",
        "            quality_score = 30\n",
        "            coherence_score = 25\n",
        "            relevance_score = 20\n",
        "        elif word_count < 30:\n",
        "            quality_score = 50\n",
        "            coherence_score = 48\n",
        "            relevance_score = 45\n",
        "        elif word_count < 50:\n",
        "            quality_score = 70\n",
        "            coherence_score = 68\n",
        "            relevance_score = 65\n",
        "        else:\n",
        "            quality_score = 85\n",
        "            coherence_score = 83\n",
        "            relevance_score = 80\n",
        "\n",
        "        tempo_bicara = 85\n",
        "        confidence_score = 82\n",
        "\n",
        "        total = round((quality_score + coherence_score + relevance_score + tempo_bicara + confidence_score) / 5)\n",
        "\n",
        "        return {\n",
        "            \"scores\": {\n",
        "                \"kualitas_jawaban\": quality_score,\n",
        "                \"koherensi\": coherence_score,\n",
        "                \"relevansi\": relevance_score,\n",
        "                \"tempo_bicara\": tempo_bicara,\n",
        "                \"confidence_score\": confidence_score\n",
        "            },\n",
        "            \"total\": total,\n",
        "            \"penilaian_akhir\": 3 if total >= 70 else 2,\n",
        "            \"cheating_detected\": \"Tidak\",\n",
        "            \"cheating_reason\": \"Tidak ada indikasi kecurangan\",\n",
        "            \"analysis\": f\"Fallback assessment based on word count ({word_count} words). Inference API evaluation failed.\",\n",
        "            \"keputusan_akhir\": \"Pertimbangan\" if total >= 70 else \"Tidak Lulus\",\n",
        "            \"scoring_method\": {\n",
        "                \"llm_evaluated\": [],\n",
        "                \"static_dummy\": [\"kualitas_jawaban\", \"koherensi\", \"relevansi\", \"tempo_bicara\", \"confidence_score\"],\n",
        "                \"fallback\": True\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_4yZAte8K6Vb",
      "metadata": {
        "id": "_4yZAte8K6Vb"
      },
      "source": [
        "<b><h2> Pembuatan Json Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "GaQ-qG9OUcm3",
      "metadata": {
        "id": "GaQ-qG9OUcm3"
      },
      "outputs": [],
      "source": [
        "def process_transcriptions_sync(session_id: str, candidate_name: str, uploaded_videos: list, base_url: str):\n",
        "    \"\"\"Background transcription processing\"\"\"\n",
        "    try:\n",
        "        print(f'\\n{\"=\"*70}')\n",
        "        print(f'üéôÔ∏è  SESSION: {session_id}')\n",
        "        print(f'üë§ CANDIDATE: {candidate_name}')\n",
        "        print(f'üìπ VIDEOS: {len(uploaded_videos)}')\n",
        "        print(f'{\"=\"*70}\\n')\n",
        "\n",
        "        transcriptions = []\n",
        "        assessment_results = []\n",
        "\n",
        "        with processing_lock:\n",
        "            processing_status[session_id] = {'status': 'processing', 'progress': '0/0'}\n",
        "\n",
        "        # Process each video with overall progress bar\n",
        "        for idx, interview in enumerate(tqdm(uploaded_videos, desc=\"üé¨ Overall Progress\", unit=\"video\", ncols=80), 1):\n",
        "            if not interview.get('isVideoExist') or not interview.get('recordedVideoUrl'):\n",
        "                transcriptions.append({\n",
        "                    'positionId': interview['positionId'],\n",
        "                    'error': interview.get('error', 'Video upload failed')\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            position_id = interview['positionId']\n",
        "            video_url = interview['recordedVideoUrl']\n",
        "            question = interview.get('question', '')\n",
        "\n",
        "            try:\n",
        "                print(f'\\n‚îå‚îÄ Video {position_id}/{len(uploaded_videos)} ‚îÄ{\"‚îÄ\"*50}‚îê')\n",
        "                if question:\n",
        "                    print(f'‚îÇ ‚ùì Question: {question[:60]}{\"...\" if len(question) > 60 else \"\"}')\n",
        "\n",
        "                local_file = get_local_file_path(video_url)\n",
        "                if not local_file:\n",
        "                    raise Exception(f\"Local file not found\")\n",
        "\n",
        "                file_size_mb = os.path.getsize(local_file) / (1024 * 1024)\n",
        "\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id] = {\n",
        "                        'status': 'processing',\n",
        "                        'progress': f'{position_id}/{len(uploaded_videos)}',\n",
        "                        'current_video': position_id,\n",
        "                        'message': f'Processing video {position_id}/{len(uploaded_videos)}...'\n",
        "                    }\n",
        "\n",
        "                video_start = time.time()\n",
        "\n",
        "                # Step 1: Transcribe\n",
        "                print(f'‚îÇ 1Ô∏è‚É£  TRANSCRIPTION ({file_size_mb:.1f} MB)')\n",
        "                transcription_text = transcribe_video(local_file)\n",
        "                transcribe_time = time.time() - video_start\n",
        "\n",
        "                # Step 2: Translate WITH CONFIDENCE SCORE\n",
        "                print(f'‚îÇ 2Ô∏è‚É£  TRANSLATION')\n",
        "                translate_start = time.time()\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id]['message'] = f'Translating video {position_id}...'\n",
        "\n",
        "                # ‚úÖ NEW: Get translation with confidence\n",
        "                translation_result = translate_to_indonesian_with_confidence(transcription_text)\n",
        "                transcription_id = translation_result['translated_text']\n",
        "                translation_confidence = translation_result['confidence_score']\n",
        "                translation_confidence_level = translation_result['confidence_level']\n",
        "                \n",
        "                translate_time = time.time() - translate_start\n",
        "                print(f'‚îÇ    üìä Translation Confidence: {translation_confidence}% ({translation_confidence_level})')\n",
        "\n",
        "                # Step 3: CHEATING DETECTION\n",
        "                print(f'‚îÇ 2Ô∏è‚É£¬Ω CHEATING DETECTION')\n",
        "                cheating_start = time.time()\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id]['message'] = f'Analyzing cheating patterns in video {position_id}...'\n",
        "\n",
        "                cheating_result = advanced_cheating_detection(local_file, transcription_text)\n",
        "                cheating_time = time.time() - cheating_start\n",
        "\n",
        "                # Step 4: NON-VERBAL ANALYSIS WITH CONFIDENCE SCORE\n",
        "                print(f'‚îÇ 2Ô∏è‚É£¬æ NON-VERBAL ANALYSIS')\n",
        "                non_verbal_start = time.time()\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id]['message'] = f'Analyzing non verbal in video {position_id}...'\n",
        "\n",
        "                # ‚úÖ NEW: Get non-verbal with confidence\n",
        "                non_verbal_result = analyze_interview_video_with_confidence(\n",
        "                    video_path=local_file,\n",
        "                    audio_path=None\n",
        "                )\n",
        "\n",
        "                non_verbal_time = time.time() - non_verbal_start\n",
        "                print(f'‚îÇ    üìä Non-Verbal Confidence: {non_verbal_result[\"confidence_score\"]}% ({non_verbal_result[\"confidence_level\"]})')\n",
        "\n",
        "                # Step 5: LLM Evaluation\n",
        "                print(f'‚îÇ 3Ô∏è‚É£  AI ASSESSMENT')\n",
        "                llm_start = time.time()\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id]['message'] = f'Evaluating video {position_id} with AI...'\n",
        "\n",
        "                llm_evaluation = evaluate_with_llm(transcription_text, question, position_id)\n",
        "                llm_time = time.time() - llm_start\n",
        "\n",
        "                # Step 6: Save\n",
        "                print(f'‚îÇ 4Ô∏è‚É£  SAVING FILES')\n",
        "                trans_fname = f\"transcription_pos{position_id}_{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex}.txt\"\n",
        "                trans_path = os.path.join(TRANSCRIPTION_DIR, trans_fname)\n",
        "\n",
        "                with open(trans_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(f\"Candidate: {candidate_name}\\n\")\n",
        "                    f.write(f\"Position ID: {position_id}\\n\")\n",
        "                    f.write(f\"Question: {question}\\n\")\n",
        "                    f.write(f\"Video URL: {video_url}\\n\")\n",
        "                    f.write(f\"Transcribed at: {datetime.now(timezone.utc).isoformat()}\\n\")\n",
        "                    f.write(f\"Model: faster-whisper large-v3\\n\")\n",
        "                    f.write(f\"Processing time: {transcribe_time:.1f}s\\n\")\n",
        "                    f.write(f\"\\n{'='*50}\\n\")\n",
        "                    f.write(f\"ENGLISH TRANSCRIPTION:\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\\n\")\n",
        "                    f.write(transcription_text)\n",
        "                    f.write(f\"\\n\\n{'='*50}\\n\")\n",
        "                    f.write(f\"INDONESIAN TRANSLATION (DeepL):\\n\")\n",
        "                    f.write(f\"Confidence: {translation_confidence}% ({translation_confidence_level})\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\\n\")\n",
        "                    f.write(transcription_id)\n",
        "                    f.write(f\"\\n\\n{'='*50}\\n\")\n",
        "                    f.write(f\"CHEATING DETECTION RESULTS:\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\\n\")\n",
        "                    f.write(json.dumps(cheating_result, indent=2, ensure_ascii=False))\n",
        "                    f.write(f\"\\n\\n{'='*50}\\n\")\n",
        "                    f.write(f\"NON-VERBAL ANALYSIS:\\n\")\n",
        "                    f.write(f\"Confidence: {non_verbal_result['confidence_score']}% ({non_verbal_result['confidence_level']})\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\\n\")\n",
        "                    f.write(json.dumps(non_verbal_result, indent=2, ensure_ascii=False))\n",
        "                    f.write(f\"\\n\\n{'='*50}\\n\")\n",
        "                    f.write(f\"AI ASSESSMENT:\\n\")\n",
        "                    f.write(f\"{'='*50}\\n\\n\")\n",
        "                    f.write(json.dumps(llm_evaluation, indent=2, ensure_ascii=False))\n",
        "\n",
        "                transcription_url = f\"{base_url}/transcriptions/{trans_fname}\"\n",
        "\n",
        "                # ‚úÖ Build final assessment\n",
        "                words = transcription_text.split()\n",
        "\n",
        "                assessment = {\n",
        "                    \"penilaian\": {\n",
        "                        \"confidence_score\": llm_evaluation['scores']['confidence_score'],\n",
        "                        \"kualitas_jawaban\": llm_evaluation['scores']['kualitas_jawaban'],\n",
        "                        \"relevansi\": llm_evaluation['scores']['relevansi'],\n",
        "                        \"koherensi\": llm_evaluation['scores']['koherensi'],\n",
        "                        \"tempo_bicara\": llm_evaluation['scores']['tempo_bicara'],\n",
        "                        \"analisis_llm\": llm_evaluation['analysis'],\n",
        "                        \"total\": llm_evaluation['total']\n",
        "                    },\n",
        "                    \"penilaian_akhir\": llm_evaluation['penilaian_akhir'],\n",
        "\n",
        "                    # Cheating Detection\n",
        "                    \"cheating_detection\": cheating_result.get('cheating_status', 'Tidak'),\n",
        "                    \"cheating_score\": cheating_result.get('cheating_score', 0),\n",
        "                    \"cheating_confidence_score\": cheating_result.get('confidence_score', 0),\n",
        "                    \"cheating_confidence_level\": cheating_result.get('confidence_level', 'N/A'),\n",
        "                    \"alasan_cheating\": ', '.join(cheating_result.get('indicators', [])) if cheating_result.get('indicators') else 'Tidak ada indikasi kecurangan',\n",
        "                    \"cheating_details\": {\n",
        "                        **cheating_result.get('details', {}),\n",
        "                        \"confidence_components\": cheating_result.get('confidence_components', {})\n",
        "                    },\n",
        "\n",
        "                    # ‚úÖ Non-Verbal Analysis (with confidence)\n",
        "                    \"non_verbal_analysis\": non_verbal_result['analysis'],\n",
        "                    \"non_verbal_confidence_score\": non_verbal_result['confidence_score'],\n",
        "                    \"non_verbal_confidence_level\": non_verbal_result['confidence_level'],\n",
        "                    \"non_verbal_confidence_components\": non_verbal_result['confidence_components'],\n",
        "\n",
        "                    \"keputusan_akhir\": llm_evaluation['keputusan_akhir'],\n",
        "                    \"transkripsi_en\": transcription_text,\n",
        "                    \"transkripsi_id\": transcription_id,\n",
        "                    \n",
        "                    # ‚úÖ Translation Confidence\n",
        "                    \"translation_confidence_score\": translation_confidence,\n",
        "                    \"translation_confidence_level\": translation_confidence_level,\n",
        "                    \n",
        "                    \"metadata\": {\n",
        "                        \"word_count\": len(words),\n",
        "                        \"char_count\": len(transcription_text),\n",
        "                        \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
        "                        \"translation_available\": True,\n",
        "                        \"llm_evaluation_time\": round(llm_time, 2),\n",
        "                        \"cheating_detection_time\": round(cheating_time, 2),\n",
        "                        \"non_verbal_analysis_time\": round(non_verbal_time, 2),\n",
        "                        \"assessment_method\": \"Hybrid (LLM + Non-Verbal + Cheating Detection)\",\n",
        "                        \"llm_evaluated_criteria\": llm_evaluation.get('scoring_method', {}).get('llm_evaluated', []),\n",
        "                        \"static_criteria\": llm_evaluation.get('scoring_method', {}).get('static_dummy', []),\n",
        "                        \"non_verbal_features\": [\"Speech Tempo\", \"Facial Expression\", \"Eye Movement\"],\n",
        "                        \"cheating_methods\": [\"Diarization\", \"Eye Detection\", \"Text Pattern\", \"Audio Quality\"],\n",
        "                        \"cheating_indicators_count\": len(cheating_result.get('indicators', [])),\n",
        "                        \"cheating_confidence_breakdown\": cheating_result.get('confidence_components', {}),\n",
        "                        \"non_verbal_confidence_breakdown\": non_verbal_result['confidence_components'],\n",
        "                        \"translation_quality_metrics\": translation_result.get('quality_metrics', {})\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                assessment_results.append({\n",
        "                    \"id\": position_id,\n",
        "                    \"question\": question,\n",
        "                    \"result\": assessment\n",
        "                })\n",
        "\n",
        "                transcriptions.append({\n",
        "                    'positionId': position_id,\n",
        "                    'question': question,\n",
        "                    'videoUrl': video_url,\n",
        "                    'transcription': transcription_text,\n",
        "                    'transcription_id': transcription_id,\n",
        "                    'transcriptionUrl': transcription_url,\n",
        "                    'transcriptionFile': trans_fname,\n",
        "                    'assessment': assessment\n",
        "                })\n",
        "\n",
        "                # Delete video\n",
        "                if os.path.exists(local_file):\n",
        "                    os.remove(local_file)\n",
        "                    print(f'‚îÇ üóëÔ∏è  Video deleted ({file_size_mb:.1f} MB freed)')\n",
        "\n",
        "                total_time = time.time() - video_start\n",
        "                print(f'‚îÇ ‚è±Ô∏è  Total: {total_time:.1f}s')\n",
        "                print(f'‚îÇ üìä Confidence Scores:')\n",
        "                print(f'‚îÇ    Translation: {translation_confidence}%')\n",
        "                print(f'‚îÇ    Non-Verbal: {non_verbal_result[\"confidence_score\"]}%')\n",
        "                print(f'‚îÇ    Cheating: {cheating_result.get(\"confidence_score\", 0)}%')\n",
        "                print(f'‚îî‚îÄ{\"‚îÄ\"*68}‚îò')\n",
        "\n",
        "                gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'‚îÇ ‚ùå ERROR: {str(e)}')\n",
        "                print(f'‚îî‚îÄ{\"‚îÄ\"*68}‚îò')\n",
        "\n",
        "                transcriptions.append({\n",
        "                    'positionId': position_id,\n",
        "                    'question': question,\n",
        "                    'videoUrl': video_url,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "\n",
        "        # ============================================================================\n",
        "        # ‚úÖ NEW: Calculate aggregate cheating analysis\n",
        "        # ============================================================================\n",
        "        aggregate_cheating = calculate_aggregate_cheating_analysis(assessment_results)\n",
        "\n",
        "        print(f'\\n{\"=\"*70}')\n",
        "        print(f'üö® AGGREGATE CHEATING ANALYSIS')\n",
        "        print(f'{\"=\"*70}')\n",
        "        print(f'Overall Status: {aggregate_cheating[\"overall_cheating_status\"]} ({aggregate_cheating[\"risk_level\"]})')\n",
        "        print(f'Confidence: {aggregate_cheating[\"confidence_level\"]}')\n",
        "        print(f'Videos Flagged: {aggregate_cheating[\"videos_flagged\"]}/{aggregate_cheating[\"total_videos\"]} ({aggregate_cheating[\"flagged_percentage\"]}%)')\n",
        "        print(f'Average Score: {aggregate_cheating[\"overall_cheating_score\"]}/100')\n",
        "        print(f'Recommendation: {aggregate_cheating[\"recommendation\"]}')\n",
        "        print(f'Summary: {aggregate_cheating[\"summary\"]}')\n",
        "        print(f'{\"=\"*70}\\n')\n",
        "\n",
        "\n",
        "        aggregate_non_verbal = summarize_non_verbal_batch(assessment_results)\n",
        "\n",
        "        # Save final results\n",
        "        if assessment_results:\n",
        "            results_json = {\n",
        "               \"success\": True,\n",
        "                \"name\": candidate_name,\n",
        "                \"session\": session_id,\n",
        "                \"content\": assessment_results,\n",
        "                \"aggregate_cheating_analysis\": aggregate_cheating,\n",
        "                \"aggregate_non_verbal_analysis\": aggregate_non_verbal,\n",
        "                \"metadata\": {\n",
        "                    \"total_videos\": len(uploaded_videos),\n",
        "                    \"successful_videos\": len(assessment_results),\n",
        "                    \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
        "                    \"model\": \"faster-whisper large-v3\",\n",
        "                    \"llm_model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "                    \"assessment_method\": \"Hybrid (LLM + Diarization + Eye Detection + Aggregate Analysis)\",\n",
        "                    \"llm_criteria\": [\"kualitas_jawaban\", \"koherensi\", \"relevansi\"],\n",
        "                    \"static_criteria\": [\"tempo_bicara\", \"confidence_score\"],\n",
        "                    \"cheating_detection_methods\": [\n",
        "                    \"Per-Video: Diarization, Eye Detection, Text Pattern, Audio Quality\",\n",
        "                    \"Aggregate: Cross-video pattern analysis, Risk scoring\"],\n",
        "                    \"videos_deleted\": True,\n",
        "                    \"translation_provider\": \"DeepL\",\n",
        "                    \"translation_language\": \"Indonesian (ID)\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "            results_filename = f\"{session_id}.json\"\n",
        "            results_path = os.path.join(RESULTS_DIR, results_filename)\n",
        "\n",
        "            with open(results_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results_json, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            results_url = f\"{base_url}/results/{results_filename}\"\n",
        "            print(f'\\nüíæ Results saved: {results_url}')\n",
        "\n",
        "        successful_count = sum(1 for t in transcriptions if 'transcription' in t)\n",
        "\n",
        "        with processing_lock:\n",
        "            processing_status[session_id] = {\n",
        "                'status': 'completed',\n",
        "                'result': {\n",
        "                    'success': True,\n",
        "                    'transcriptions': transcriptions,\n",
        "                    'processed_videos': len(transcriptions),\n",
        "                    'successful_videos': successful_count,\n",
        "                    'failed_videos': len(transcriptions) - successful_count,\n",
        "                    'results_url': f\"{base_url}/results/{session_id}.json\" if assessment_results else None\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(f'\\n{\"=\"*70}')\n",
        "        print(f'‚úÖ SESSION COMPLETED')\n",
        "        print(f'   Success: {successful_count}/{len(transcriptions)} videos')\n",
        "        print(f'{\"=\"*70}\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'\\n‚ùå SESSION ERROR:\\n{traceback.format_exc()}')\n",
        "\n",
        "        with processing_lock:\n",
        "            processing_status[session_id] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'error_detail': traceback.format_exc()\n",
        "            }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bBkV62fsK-FO",
      "metadata": {
        "id": "bBkV62fsK-FO"
      },
      "source": [
        "<b><h2> ENDPOINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "qLpXqXHoUhWr",
      "metadata": {
        "id": "qLpXqXHoUhWr"
      },
      "outputs": [],
      "source": [
        "# ENDPOINTS\n",
        "@app.post('/upload')\n",
        "async def receive_videos_and_process(\n",
        "    request: Request,\n",
        "    candidate_name: str = Form(...),\n",
        "    videos: List[UploadFile] = File(...),\n",
        "    questions: List[str] = Form(...)  # NEW: Accept questions array\n",
        "):\n",
        "    \"\"\"Upload videos and start background transcription\"\"\"\n",
        "    session_id = uuid.uuid4().hex\n",
        "    print(f'\\nüîµ NEW UPLOAD REQUEST - Session: {session_id}')\n",
        "    print(f'   Candidate: {candidate_name}')\n",
        "    print(f'   Videos: {len(videos)} file(s)')\n",
        "    print(f'   Questions: {len(questions)} question(s)')  # NEW\n",
        "\n",
        "    # NEW: Validate questions count matches videos count\n",
        "    if len(questions) != len(videos):\n",
        "        return JSONResponse(\n",
        "            content={\n",
        "                'success': False,\n",
        "                'error': f'Questions count ({len(questions)}) must match videos count ({len(videos)})'\n",
        "            },\n",
        "            status_code=400,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Initialize status FIRST\n",
        "    with processing_lock:\n",
        "        processing_status[session_id] = {\n",
        "            'status': 'uploading',\n",
        "            'progress': '0/0',\n",
        "            'message': 'Uploading videos...'\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # 1. Upload semua video (fast)\n",
        "        base_url = str(request.base_url).rstrip('/')\n",
        "        uploaded_videos = []\n",
        "\n",
        "        print(f'\\nüì§ Uploading {len(videos)} video(s)...')\n",
        "        for idx, (video, question) in enumerate(zip(videos, questions), 1):  # NEW: zip with questions\n",
        "            try:\n",
        "                ext = os.path.splitext(video.filename)[1] or '.webm'\n",
        "                safe_name = f\"{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex}{ext}\"\n",
        "                dest_path = os.path.join(UPLOAD_DIR, safe_name)\n",
        "\n",
        "                # Update upload progress\n",
        "                with processing_lock:\n",
        "                    processing_status[session_id]['message'] = f'Uploading video {idx}/{len(videos)}...'\n",
        "                    processing_status[session_id]['progress'] = f'{idx}/{len(videos)}'\n",
        "\n",
        "                with open(dest_path, 'wb') as buffer:\n",
        "                    shutil.copyfileobj(video.file, buffer)\n",
        "\n",
        "                file_url = f\"{base_url}/uploads/{safe_name}\"\n",
        "                uploaded_videos.append({\n",
        "                    'positionId': idx,\n",
        "                    'question': question,  # NEW: Include question\n",
        "                    'isVideoExist': True,\n",
        "                    'recordedVideoUrl': file_url,\n",
        "                    'filename': safe_name\n",
        "                })\n",
        "                print(f'   ‚úÖ Uploaded: {safe_name} | Q: {question[:50]}{\"...\" if len(question) > 50 else \"\"}')  # NEW\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f'   ‚ùå Failed: {str(e)}')\n",
        "                uploaded_videos.append({\n",
        "                    'positionId': idx,\n",
        "                    'question': question if idx <= len(questions) else '',  # NEW: Include question even on error\n",
        "                    'isVideoExist': False,\n",
        "                    'recordedVideoUrl': None,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "        # 2. Update status to processing\n",
        "        with processing_lock:\n",
        "            processing_status[session_id] = {\n",
        "                'status': 'processing',\n",
        "                'progress': '0/' + str(len(uploaded_videos)),\n",
        "                'message': 'Starting transcription...',\n",
        "                'uploaded_videos': len(uploaded_videos)\n",
        "            }\n",
        "\n",
        "        # 3. Start background thread\n",
        "        thread = th.Thread(\n",
        "            target=process_transcriptions_sync,\n",
        "            args=(session_id, candidate_name, uploaded_videos, base_url),\n",
        "            daemon=True\n",
        "        )\n",
        "        thread.start()\n",
        "\n",
        "        print(f'‚úÖ Upload complete. Background thread started.')\n",
        "        print(f'üì§ Returning immediate response with session_id: {session_id}')\n",
        "\n",
        "        # 4. RETURN IMMEDIATELY - no waiting!\n",
        "        return JSONResponse(\n",
        "            content={\n",
        "                'success': True,\n",
        "                'session_id': session_id,\n",
        "                'message': 'Videos uploaded successfully. Processing started.',\n",
        "                'uploaded_videos': len(uploaded_videos)\n",
        "            },\n",
        "            status_code=200,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "            }\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_detail = traceback.format_exc()\n",
        "        print(f'‚ùå Error:\\n{error_detail}')\n",
        "\n",
        "        # Update status to error\n",
        "        with processing_lock:\n",
        "            processing_status[session_id] = {\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'error_detail': error_detail\n",
        "            }\n",
        "\n",
        "        return JSONResponse(\n",
        "            content={\n",
        "                'success': False,\n",
        "                'session_id': session_id,\n",
        "                'error': str(e)\n",
        "            },\n",
        "            status_code=500,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "JCMdYEh2Umes",
      "metadata": {
        "id": "JCMdYEh2Umes"
      },
      "outputs": [],
      "source": [
        "@app.get('/status/{session_id}')\n",
        "async def get_processing_status(session_id: str):\n",
        "    \"\"\"Check processing status\"\"\"\n",
        "    with processing_lock:\n",
        "        if session_id not in processing_status:\n",
        "            return JSONResponse(\n",
        "                {\n",
        "                    'status': 'not_found',\n",
        "                    'message': 'Session not found'\n",
        "                },\n",
        "                status_code=404,\n",
        "                headers={\n",
        "                    'Access-Control-Allow-Origin': '*',\n",
        "                    'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
        "                    'Access-Control-Allow-Headers': '*',\n",
        "                    'Cache-Control': 'no-cache, no-store, must-revalidate',\n",
        "                }\n",
        "            )\n",
        "\n",
        "        status_copy = processing_status[session_id].copy()\n",
        "\n",
        "    # Add redirect URL if completed\n",
        "    if status_copy.get('status') == 'completed':\n",
        "        status_copy['redirect'] = f\"halaman_dasboard.html?session={session_id}\"\n",
        "\n",
        "    return JSONResponse(\n",
        "        status_copy,\n",
        "        headers={\n",
        "            'Access-Control-Allow-Origin': '*',\n",
        "            'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
        "            'Access-Control-Allow-Headers': '*',\n",
        "            'Cache-Control': 'no-cache, no-store, must-revalidate',\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "pHHf1yApUoi_",
      "metadata": {
        "id": "pHHf1yApUoi_"
      },
      "outputs": [],
      "source": [
        "@app.get('/results/{session_id}')\n",
        "async def get_results(session_id: str):\n",
        "    \"\"\"Get assessment results for a session\"\"\"\n",
        "    results_filename = f\"{session_id}.json\"\n",
        "    results_path = os.path.join(RESULTS_DIR, results_filename)\n",
        "\n",
        "    if not os.path.exists(results_path):\n",
        "        return JSONResponse(\n",
        "            {\n",
        "                'success': False,\n",
        "                'message': 'Results not found for this session',\n",
        "                'session_id': session_id\n",
        "            },\n",
        "            status_code=404,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "            }\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        with open(results_path, 'r', encoding='utf-8') as f:\n",
        "            results_data = json.load(f)\n",
        "\n",
        "        return JSONResponse(\n",
        "            results_data,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "                'Cache-Control': 'no-cache, no-store, must-revalidate',\n",
        "            }\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return JSONResponse(\n",
        "            {\n",
        "                'success': False,\n",
        "                'message': f'Error reading results: {str(e)}',\n",
        "                'session_id': session_id\n",
        "            },\n",
        "            status_code=500,\n",
        "            headers={\n",
        "                'Access-Control-Allow-Origin': '*',\n",
        "                'Access-Control-Allow-Methods': 'GET, OPTIONS',\n",
        "                'Access-Control-Allow-Headers': '*',\n",
        "            }\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "fa761b7c",
      "metadata": {
        "id": "fa761b7c"
      },
      "outputs": [],
      "source": [
        "@app.get('/')\n",
        "async def index():\n",
        "    return {\n",
        "        'message': 'AI Interview Assessment System',\n",
        "        'model': 'faster-whisper large-v3',\n",
        "        'accuracy': '98%+ for clear English speech',\n",
        "        'speed': '4-5x faster than standard Whisper',\n",
        "        'endpoints': {\n",
        "            'upload': 'POST /upload',\n",
        "            'status': 'GET /status/{session_id}',\n",
        "            'results': 'GET /results/{session_id}',\n",
        "            'test_form': 'GET /upload_form'\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_hOAn3rLBAV",
      "metadata": {
        "id": "I_hOAn3rLBAV"
      },
      "source": [
        "<b><h2> LOCAL SERVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "5aa6ad00",
      "metadata": {
        "id": "5aa6ad00",
        "outputId": "1e6fd650-65b6-44ed-fea7-362ea8474005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∏Ô∏è  Stopping previous server...\n",
            "‚úÖ Previous server stopped.\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "üöÄ Server started successfully!\n",
            "üìç Local URL: http://127.0.0.1:8888\n",
            "üìç Network URL: http://0.0.0.0:8888\n",
            "üîß Endpoints:\n",
            "   - POST /upload       (upload videos & process)\n",
            "   - POST /upload_json  (upload JSON & download videos)\n",
            "   - GET  /status/{id}  (check processing status)\n",
            "   - GET  /results/{id} (get assessment results)\n",
            "   - GET  /upload_form  (test form)\n",
            "‚ÑπÔ∏è  Use Interrupt Kernel to stop the server\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
          ]
        }
      ],
      "source": [
        "# Jalankan server uvicorn di dalam notebook (tanpa ngrok)\n",
        "nest_asyncio.apply()\n",
        "PORT = 8888\n",
        "\n",
        "# Hentikan server sebelumnya jika ada\n",
        "if 'server_thread' in globals() and server_thread is not None:\n",
        "    try:\n",
        "        print('‚è∏Ô∏è  Stopping previous server...')\n",
        "        if 'server' in globals() and server is not None:\n",
        "            server.should_exit = True\n",
        "        # Tunggu thread selesai (dengan timeout)\n",
        "        if server_thread.is_alive():\n",
        "            server_thread.join(timeout=2)\n",
        "        print('‚úÖ Previous server stopped.')\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è  Error stopping previous server: {e}')\n",
        "\n",
        "# Buat server instance baru dengan log level yang lebih rendah\n",
        "config = uvicorn.Config(\n",
        "    app=app,\n",
        "    host='0.0.0.0',\n",
        "    port=PORT,\n",
        "    log_level='warning',  # Kurangi verbosity untuk menghindari duplikasi log\n",
        "    access_log=False  # Nonaktifkan access log di console\n",
        ")\n",
        "server = uvicorn.Server(config=config)\n",
        "\n",
        "# Fungsi untuk menjalankan server di thread\n",
        "def run_server_in_thread():\n",
        "    # Buat event loop baru untuk thread ini\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        loop.run_until_complete(server.serve())\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Server error: {e}')\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "# Jalankan server di background thread\n",
        "server_thread = threading.Thread(target=run_server_in_thread, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print('‚îÅ' * 60)\n",
        "print('üöÄ Server started successfully!')\n",
        "print(f'üìç Local URL: http://127.0.0.1:{PORT}')\n",
        "print(f'üìç Network URL: http://0.0.0.0:{PORT}')\n",
        "print(f'üîß Endpoints:')\n",
        "print(f'   - POST /upload       (upload videos & process)')\n",
        "print(f'   - POST /upload_json  (upload JSON & download videos)')\n",
        "print(f'   - GET  /status/{{id}}  (check processing status)')\n",
        "print(f'   - GET  /results/{{id}} (get assessment results)')\n",
        "print(f'   - GET  /upload_form  (test form)')\n",
        "print('‚ÑπÔ∏è  Use Interrupt Kernel to stop the server')\n",
        "print('‚îÅ' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t5wi7BtWLDbW",
      "metadata": {
        "id": "t5wi7BtWLDbW"
      },
      "source": [
        "<b><h2> NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "5e9bb1c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9bb1c3",
        "outputId": "b4b6b991-9627-40de-f298-931a27b07efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ngrok configured successfully\n"
          ]
        }
      ],
      "source": [
        "# Configure ngrok\n",
        "# Set ngrok authtoken (dapatkan dari https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "NGROK_AUTH_TOKEN = getpass.getpass('Enter your ngrok authtoken: ')\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "\n",
        "print('‚úÖ Ngrok configured successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38392051",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38392051",
        "outputId": "b2ceaec4-74a2-4cf9-a0c0-153ccd6e04d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∏Ô∏è  Stopping previous server...\n",
            "‚úÖ Previous server stopped.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-12-03T08:35:46+0700 lvl=eror msg=\"unable to evaluate ngrok agent binary path for symlinks\" obj=tunnels.session err=\"CreateFile C:\\\\Users\\\\NFSYNX\\\\AppData\\\\Local\\\\ngrok: The system cannot find the file specified.\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
            "üöÄ Server started successfully with ngrok!\n",
            "üìç Local URL: http://127.0.0.1:8888\n",
            "üåê Public URL (ngrok): https://redemptory-lavern-fiendishly.ngrok-free.dev\n",
            "üìã Copy this URL to use in Upload.js:\n",
            "   const VIDEO_ENDPOINT = \"https://redemptory-lavern-fiendishly.ngrok-free.dev/upload\";\n",
            "üìß Endpoints:\n",
            "   - POST https://redemptory-lavern-fiendishly.ngrok-free.dev/upload\n",
            "   - GET  https://redemptory-lavern-fiendishly.ngrok-free.dev/status/{id}\n",
            "   - GET  https://redemptory-lavern-fiendishly.ngrok-free.dev/results/{id}\n",
            "   - GET  https://redemptory-lavern-fiendishly.ngrok-free.dev/upload_form\n",
            "‚ÑπÔ∏è  Ngrok tunnel will stay active while notebook is running\n",
            "‚ÑπÔ∏è  Use Interrupt Kernel to stop the server\n",
            "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîµ NEW UPLOAD REQUEST - Session: a59dfd4cf38a4349ad097976cb8d5ac3\n",
            "   Candidate: test\n",
            "   Videos: 2 file(s)\n",
            "   Questions: 2 question(s)\n",
            "\n",
            "üì§ Uploading 2 video(s)...\n",
            "   ‚úÖ Uploaded: 20251203013630_517eb07acb3e4afea3b79aac4f68db58.webm | Q: Can you share any specific challenges you faced wh...\n",
            "   ‚úÖ Uploaded: 20251203013630_2ad2ae136bee4f47be1a23f34e0be1a4.webm | Q: Can you describe your experience with transfer lea...\n",
            "\n",
            "======================================================================\n",
            "üéôÔ∏è  SESSION: a59dfd4cf38a4349ad097976cb8d5ac3\n",
            "üë§ CANDIDATE: test\n",
            "üìπ VIDEOS: 2\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Upload complete. Background thread started.\n",
            "üì§ Returning immediate response with session_id: a59dfd4cf38a4349ad097976cb8d5ac3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üé¨ Overall Progress:   0%|                             | 0/2 [00:00<?, ?video/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚îå‚îÄ Video 1/2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ ‚ùì Question: Can you share any specific challenges you faced while workin...\n",
            "‚îÇ 1Ô∏è‚É£  TRANSCRIPTION (17.1 MB)\n",
            "üìÅ Video: 20251203013630_517eb07acb3e4afea3b79aac4f68db58.webm (17.12 MB)\n",
            "üîÑ Starting transcription...\n",
            "   üìù Collecting segments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üßπ Cleaned: 763 ‚Üí 755 chars\n",
            "   ‚úÖ Completed in 51.9s | 9 segments | 128 words\n",
            "‚îÇ 2Ô∏è‚É£  TRANSLATION\n",
            "   ‚úÖ Translation: 755 ‚Üí 817 chars\n",
            "   üìä Confidence: 98% (Very High)\n",
            "      Length: 100% | Speed: 90% | API: 100% | Coverage: 100%\n",
            "‚îÇ    üìä Translation Confidence: 98% (Very High)\n",
            "‚îÇ 2Ô∏è‚É£¬Ω CHEATING DETECTION\n",
            "   üö® Advanced Cheating Detection:\n",
            "   ‚îÇ 1Ô∏è‚É£  Speaker Diarization Check\n",
            "   üé§ Performing speaker diarization (Silero VAD)...\n",
            "   ‚îÇ ‚úÖ Silero VAD model loaded\n",
            "   ‚îÇ Attempting to load audio...\n",
            "   ‚îÇ ‚ö†Ô∏è  torchaudio load failed: Could not load libtorchcodec. Likely causes:\n",
            "     \n",
            "   ‚îÇ Fallback: Using pydub to extract audio...\n",
            "   ‚îÇ ‚úÖ Audio extracted via pydub: 1 channels @ 16000Hz\n",
            "   ‚îÇ ‚ÑπÔ∏è  Audio duration: 93.0s\n",
            "   ‚îÇ Analyzing speech patterns...\n",
            "   ‚îÇ ‚ÑπÔ∏è  Detected 161 speech segments\n",
            "   ‚îÇ ‚ÑπÔ∏è  Avg segment: 0.1s | Long pauses: 10\n",
            "   ‚îÇ ‚úÖ Analysis complete: 1 speaker(s)\n",
            "   ‚îÇ    Confidence: MEDIUM\n",
            "   ‚îÇ    Reasoning: Monologue pattern\n",
            "   ‚îÇ    üìä Diarization: 74% (base: 70, quality: 85)\n",
            "   ‚îÇ    ‚úÖ Single speaker confirmed\n",
            "   ‚îÇ 2Ô∏è‚É£  Eye Detection & Gaze Analysis\n",
            "   üëÅÔ∏è  Eye detection analysis...\n",
            "   ‚îÇ FPS: 16.6 | Total Frames: -152954252944508352 | Interval: 3\n",
            "\n",
            "   ‚úÖ Eye Detection Complete:\n",
            "      Face: 33.4% | Eyes open: 515 | Eyes closed: 0\n",
            "   ‚îÇ    üìä Eye Detection: 75% (base: 55, coverage: 100, visibility: 90)\n",
            "   ‚îÇ    ‚ö†Ô∏è  Downward gaze (-15 points)\n",
            "   ‚îÇ 3Ô∏è‚É£  Text Pattern Analysis\n",
            "   ‚îÇ    üìä Text Pattern: 83% (base: 95, diversity: 65)\n",
            "   ‚îÇ    ‚úÖ Text pattern normal\n",
            "   ‚îÇ 4Ô∏è‚É£  Audio Quality Check\n",
            "   ‚îÇ    üìä Audio Quality: 95% (SNR: 804.8, noise: 1.2)\n",
            "   ‚îÇ    ‚úÖ Audio quality normal (noise: 1.2)\n",
            "   ‚îÇ üìä Final Cheating Score: 15/100\n",
            "   ‚îÇ üéØ Overall Confidence: 90.5% (Very High)\n",
            "   ‚îÇ üö® Cheating Detection: Tidak\n",
            "   ‚îÇ ‚ö†Ô∏è  Indicators (1):\n",
            "   ‚îÇ    - Eye detection: Frequent downward gaze\n",
            "‚îÇ 2Ô∏è‚É£¬æ NON-VERBAL ANALYSIS\n",
            "üé¨ Memulai analisis interview...\n",
            "üì§ Mengekstrak audio dari video...\n",
            "   ‚è≥ Mengekstrak audio dari d:\\Interview_Assesment_System-ngrok-raifal\\uploads\\20251203013630_517eb07acb3e4afea3b79aac4f68db58.webm...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\NFSYNX\\AppData\\Local\\Temp\\ipykernel_9012\\333445217.py:183: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(video_path, sr=16000, duration=30)\n",
            "d:\\Interview_Assesment_System-ngrok-raifal\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Audio berhasil diekstrak: d:\\Interview_Assesment_System-ngrok-raifal\\audio\\20251203013630_517eb07acb3e4afea3b79aac4f68db58.wav\n",
            "üé§ Analisis tempo bicara...\n",
            "üòä Analisis ekspresi wajah...\n",
            "üëÅÔ∏è Analisis gerakan mata...\n",
            "\n",
            "‚úÖ Non-Verbal Analysis Complete\n",
            "   Confidence: 87% (Very High)\n",
            "   Components: Speech=95%, Face=95%, Eye=60%, Duration=100%\n",
            "\n",
            "‚îÇ    üìä Non-Verbal Confidence: 87% (Very High)\n",
            "‚îÇ 3Ô∏è‚É£  AI ASSESSMENT\n",
            "‚îÇ ü§ñ Llama-3.1 Inference API Evaluation (3 criteria)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üé¨ Overall Progress:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 1/2 [01:24<01:24, 84.76s/video]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚îÇ üì® API Response received (583 chars)\n",
            "‚îÇ üìä LLM Scores: Quality=60, Coherence=30, Relevance=40\n",
            "‚îÇ üìå Static: Tempo=85, Confidence=82\n",
            "‚îÇ ‚úÖ Total Score: 59/100 | Rating: 1/5 | Decision: Tidak Lulus\n",
            "‚îÇ 4Ô∏è‚É£  SAVING FILES\n",
            "‚îÇ üóëÔ∏è  Video deleted (17.1 MB freed)\n",
            "‚îÇ ‚è±Ô∏è  Total: 84.6s\n",
            "‚îÇ üìä Confidence Scores:\n",
            "‚îÇ    Translation: 98%\n",
            "‚îÇ    Non-Verbal: 87%\n",
            "‚îÇ    Cheating: 90.5%\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "‚îå‚îÄ Video 2/2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ ‚ùì Question: Can you describe your experience with transfer learning in T...\n",
            "‚îÇ 1Ô∏è‚É£  TRANSCRIPTION (21.0 MB)\n",
            "üìÅ Video: 20251203013630_2ad2ae136bee4f47be1a23f34e0be1a4.webm (20.96 MB)\n",
            "üîÑ Starting transcription...\n",
            "   üìù Collecting segments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üßπ Cleaned: 1008 ‚Üí 992 chars\n",
            "   ‚úÖ Completed in 76.3s | 17 segments | 163 words\n",
            "‚îÇ 2Ô∏è‚É£  TRANSLATION\n",
            "   ‚úÖ Translation: 992 ‚Üí 1109 chars\n",
            "   üìä Confidence: 98% (Very High)\n",
            "      Length: 100% | Speed: 90% | API: 100% | Coverage: 100%\n",
            "‚îÇ    üìä Translation Confidence: 98% (Very High)\n",
            "‚îÇ 2Ô∏è‚É£¬Ω CHEATING DETECTION\n",
            "   üö® Advanced Cheating Detection:\n",
            "   ‚îÇ 1Ô∏è‚É£  Speaker Diarization Check\n",
            "   üé§ Performing speaker diarization (Silero VAD)...\n",
            "   ‚îÇ ‚úÖ Silero VAD model loaded\n",
            "   ‚îÇ Attempting to load audio...\n",
            "   ‚îÇ ‚ö†Ô∏è  torchaudio load failed: Could not load libtorchcodec. Likely causes:\n",
            "     \n",
            "   ‚îÇ Fallback: Using pydub to extract audio...\n",
            "   ‚îÇ ‚úÖ Audio extracted via pydub: 1 channels @ 16000Hz\n",
            "   ‚îÇ ‚ÑπÔ∏è  Audio duration: 114.7s\n",
            "   ‚îÇ Analyzing speech patterns...\n",
            "   ‚îÇ ‚ÑπÔ∏è  Detected 258 speech segments\n",
            "   ‚îÇ ‚ÑπÔ∏è  Avg segment: 0.1s | Long pauses: 7\n",
            "   ‚îÇ ‚úÖ Analysis complete: 1 speaker(s)\n",
            "   ‚îÇ    Confidence: MEDIUM\n",
            "   ‚îÇ    Reasoning: Monologue pattern\n",
            "   ‚îÇ    üìä Diarization: 74% (base: 70, quality: 85)\n",
            "   ‚îÇ    ‚úÖ Single speaker confirmed\n",
            "   ‚îÇ 2Ô∏è‚É£  Eye Detection & Gaze Analysis\n",
            "   üëÅÔ∏è  Eye detection analysis...\n",
            "   ‚îÇ FPS: 16.7 | Total Frames: -153722867280912960 | Interval: 3\n",
            "\n",
            "   ‚úÖ Eye Detection Complete:\n",
            "      Face: 33.4% | Eyes open: 636 | Eyes closed: 0\n",
            "   ‚îÇ    üìä Eye Detection: 75% (base: 55, coverage: 100, visibility: 90)\n",
            "   ‚îÇ    ‚ö†Ô∏è  Downward gaze (-15 points)\n",
            "   ‚îÇ 3Ô∏è‚É£  Text Pattern Analysis\n",
            "   ‚îÇ    üìä Text Pattern: 79% (base: 95, diversity: 57)\n",
            "   ‚îÇ    ‚úÖ Text pattern normal\n",
            "   ‚îÇ 4Ô∏è‚É£  Audio Quality Check\n",
            "   ‚îÇ    üìä Audio Quality: 95% (SNR: 692.9, noise: 1.2)\n",
            "   ‚îÇ    ‚úÖ Audio quality normal (noise: 1.2)\n",
            "   ‚îÇ üìä Final Cheating Score: 15/100\n",
            "   ‚îÇ üéØ Overall Confidence: 89.3% (Very High)\n",
            "   ‚îÇ üö® Cheating Detection: Tidak\n",
            "   ‚îÇ ‚ö†Ô∏è  Indicators (1):\n",
            "   ‚îÇ    - Eye detection: Frequent downward gaze\n",
            "‚îÇ 2Ô∏è‚É£¬æ NON-VERBAL ANALYSIS\n",
            "üé¨ Memulai analisis interview...\n",
            "üì§ Mengekstrak audio dari video...\n",
            "   ‚è≥ Mengekstrak audio dari d:\\Interview_Assesment_System-ngrok-raifal\\uploads\\20251203013630_2ad2ae136bee4f47be1a23f34e0be1a4.webm...\n",
            "   ‚úÖ Audio berhasil diekstrak: d:\\Interview_Assesment_System-ngrok-raifal\\audio\\20251203013630_2ad2ae136bee4f47be1a23f34e0be1a4.wav\n",
            "üé§ Analisis tempo bicara...\n",
            "üòä Analisis ekspresi wajah...\n",
            "üëÅÔ∏è Analisis gerakan mata...\n",
            "\n",
            "‚úÖ Non-Verbal Analysis Complete\n",
            "   Confidence: 87% (Very High)\n",
            "   Components: Speech=95%, Face=95%, Eye=60%, Duration=100%\n",
            "\n",
            "‚îÇ    üìä Non-Verbal Confidence: 87% (Very High)\n",
            "‚îÇ 3Ô∏è‚É£  AI ASSESSMENT\n",
            "‚îÇ ü§ñ Llama-3.1 Inference API Evaluation (3 criteria)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üé¨ Overall Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [03:25<00:00, 102.58s/video]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚îÇ üì® API Response received (551 chars)\n",
            "‚îÇ üìä LLM Scores: Quality=60, Coherence=40, Relevance=70\n",
            "‚îÇ üìå Static: Tempo=85, Confidence=82\n",
            "‚îÇ ‚úÖ Total Score: 67/100 | Rating: 2/5 | Decision: Tidak Lulus\n",
            "‚îÇ 4Ô∏è‚É£  SAVING FILES\n",
            "‚îÇ üóëÔ∏è  Video deleted (21.0 MB freed)\n",
            "‚îÇ ‚è±Ô∏è  Total: 120.3s\n",
            "‚îÇ üìä Confidence Scores:\n",
            "‚îÇ    Translation: 98%\n",
            "‚îÇ    Non-Verbal: 87%\n",
            "‚îÇ    Cheating: 89.3%\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "======================================================================\n",
            "üö® AGGREGATE CHEATING ANALYSIS\n",
            "======================================================================\n",
            "Overall Status: Tidak (LOW RISK)\n",
            "Confidence: High\n",
            "Videos Flagged: 0/2 (0.0%)\n",
            "Average Score: 15.0/100\n",
            "Recommendation: LULUS - No significant cheating indicators\n",
            "Summary: Tidak ditemukan indikasi kecurangan yang signifikan di semua video.\n",
            "======================================================================\n",
            "\n",
            "\n",
            "üíæ Results saved: https://redemptory-lavern-fiendishly.ngrok-free.dev/results/a59dfd4cf38a4349ad097976cb8d5ac3.json\n",
            "\n",
            "======================================================================\n",
            "‚úÖ SESSION COMPLETED\n",
            "   Success: 2/2 videos\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Start server with ngrok\n",
        "nest_asyncio.apply()\n",
        "PORT = 8888\n",
        "\n",
        "# Stop previous server if exists\n",
        "if 'server_thread' in globals() and server_thread is not None:\n",
        "    try:\n",
        "        print('‚è∏Ô∏è  Stopping previous server...')\n",
        "        if 'server' in globals() and server is not None:\n",
        "            server.should_exit = True\n",
        "        if server_thread.is_alive():\n",
        "            server_thread.join(timeout=2)\n",
        "        print('‚úÖ Previous server stopped.')\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è  Error stopping previous server: {e}')\n",
        "\n",
        "# Close previous ngrok tunnels\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create server instance\n",
        "config = uvicorn.Config(\n",
        "    app=app,\n",
        "    host='0.0.0.0',\n",
        "    port=PORT,\n",
        "    log_level='warning',\n",
        "    access_log=False\n",
        ")\n",
        "server = uvicorn.Server(config=config)\n",
        "\n",
        "# Run server in thread\n",
        "def run_server_in_thread():\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        loop.run_until_complete(server.serve())\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Server error: {e}')\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "server_thread = threading.Thread(target=run_server_in_thread, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(2)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(PORT, bind_tls=True)\n",
        "ngrok_url = public_url.public_url\n",
        "\n",
        "print('‚îè' + '‚îÅ' * 70 + '‚îì')\n",
        "print('üöÄ Server started successfully with ngrok!')\n",
        "print(f'üìç Local URL: http://127.0.0.1:{PORT}')\n",
        "print(f'üåê Public URL (ngrok): {ngrok_url}')\n",
        "print(f'üìã Copy this URL to use in Upload.js:')\n",
        "print(f'   const VIDEO_ENDPOINT = \"{ngrok_url}/upload\";')\n",
        "print(f'üìß Endpoints:')\n",
        "print(f'   - POST {ngrok_url}/upload')\n",
        "print(f'   - GET  {ngrok_url}/status/{{id}}')\n",
        "print(f'   - GET  {ngrok_url}/results/{{id}}')\n",
        "print(f'   - GET  {ngrok_url}/upload_form')\n",
        "print('‚ÑπÔ∏è  Ngrok tunnel will stay active while notebook is running')\n",
        "print('‚ÑπÔ∏è  Use Interrupt Kernel to stop the server')\n",
        "print('‚îó' + '‚îÅ' * 70 + '‚îõ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f94a342",
      "metadata": {
        "id": "5f94a342"
      },
      "source": [
        "## System Information\n",
        "\n",
        "### Whisper Model\n",
        "- **Library**: `faster-whisper` (optimized implementation)\n",
        "- **Model**: `large-v3` (most accurate available)\n",
        "- **Accuracy**: ~98% for clear English speech\n",
        "- **Speed**: 4-5x faster than `openai-whisper`\n",
        "\n",
        "### Translation\n",
        "- **Provider**: DeepL API\n",
        "- **Target Language**: Indonesian (ID)\n",
        "- **Source Language**: English (EN)\n",
        "- **Character Limit**: 5,000 per chunk\n",
        "- **Setup**: Set `DEEPL_API_KEY` in cell 4\n",
        "- **Get API Key**: https://www.deepl.com/pro-api (Free tier: 500,000 chars/month)\n",
        "\n",
        "### LLM Assessment\n",
        "- **Model**: meta-llama/Llama-2-7b-chat-hf\n",
        "- **Method**: Hybrid (LLM + Static)\n",
        "- **LLM Evaluated Criteria** (3):\n",
        "  1. **Kualitas Jawaban** - Quality of answer (clarity, completeness, depth)\n",
        "  2. **Koherensi** - Coherence (logical flow, consistency, structure)\n",
        "  3. **Relevansi** - Relevance (alignment with question, staying on topic)\n",
        "- **Static Dummy Values** (2):\n",
        "  4. **Tempo Bicara** - Speaking tempo (fixed at 85/100) üîß *TODO: Replace with audio analysis model*\n",
        "  5. **Confidence Score** - Confidence (fixed at 82/100) üîß *TODO: Replace with voice analysis model*\n",
        "- **Cheating Detection**: LLM analyzes for multiple speakers, artificial voice, reading patterns\n",
        "- **Fallback**: Rule-based assessment if LLM fails\n",
        "\n",
        "### Performance\n",
        "- **Device**: Automatically detects CUDA GPU (if available) or CPU\n",
        "- **Compute Type**:\n",
        "  - GPU: `float16` (faster with high accuracy)\n",
        "  - CPU: `int8` (optimized for CPU)\n",
        "- **VAD Filter**: Enabled (skips silence for efficiency)\n",
        "\n",
        "### Settings\n",
        "- **Beam Size**: 5 (higher = more accurate)\n",
        "- **Best Of**: 5 (samples multiple candidates)\n",
        "- **Patience**: 2.0 (thorough beam search)\n",
        "- **Temperature**: 0.0 (deterministic output)\n",
        "- **Context**: Uses previous text for better accuracy\n",
        "\n",
        "### Storage Management\n",
        "- **Auto-delete videos**: ‚úÖ Videos are automatically deleted after successful transcription\n",
        "- **Storage saved**: Only transcriptions and results are kept\n",
        "- **Safety**: Deletion only happens after successful transcription\n",
        "- **Error handling**: If deletion fails, processing continues normally\n",
        "\n",
        "### Endpoints\n",
        "- `POST /upload` - Upload videos and start transcription\n",
        "- `GET /status/{session_id}` - Check processing status\n",
        "- **`GET /results/{session_id}`** - **Get assessment results**\n",
        "- `GET /upload_form` - Test form interface\n",
        "- `GET /` - System information\n",
        "\n",
        "### Files\n",
        "- ~~Uploaded videos: `uploads/`~~ (deleted after transcription) ‚ôªÔ∏è\n",
        "- Transcriptions: `transcriptions/` ‚úÖ (includes English + Indonesian + Assessment)\n",
        "- **Assessment results: `results/`** ‚úÖ\n",
        "\n",
        "### Assessment Data Structure\n",
        "```json\n",
        "{\n",
        "  \"success\": true,\n",
        "  \"name\": \"Candidate Name\",\n",
        "  \"session\": \"session_id_here\",\n",
        "  \"content\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"question\": \"What is your experience with Python?\",\n",
        "      \"result\": {\n",
        "        \"penilaian\": {\n",
        "          \"kualitas_jawaban\": 85,    // ‚úÖ LLM evaluated\n",
        "          \"koherensi\": 83,            // ‚úÖ LLM evaluated\n",
        "          \"relevansi\": 80,            // ‚úÖ LLM evaluated\n",
        "          \"tempo_bicara\": 85,         // üîß Static dummy (TODO: audio model)\n",
        "          \"confidence_score\": 82,     // üîß Static dummy (TODO: voice model)\n",
        "          \"total\": 83\n",
        "        },\n",
        "        \"penilaian_akhir\": 4,\n",
        "        \"cheating_detection\": \"Tidak\",\n",
        "        \"keputusan_akhir\": \"Lulus\",\n",
        "        \"transkripsi_en\": \"...\",\n",
        "        \"transkripsi_id\": \"...\",\n",
        "        \"metadata\": {\n",
        "          \"assessment_method\": \"Hybrid (LLM + Static)\",\n",
        "          \"llm_evaluated_criteria\": [\"kualitas_jawaban\", \"koherensi\", \"relevansi\"],\n",
        "          \"static_criteria\": [\"tempo_bicara\", \"confidence_score\"]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"metadata\": {\n",
        "    \"assessment_method\": \"Hybrid (LLM + Static)\",\n",
        "    \"llm_criteria\": [\"kualitas_jawaban\", \"koherensi\", \"relevansi\"],\n",
        "    \"static_criteria\": [\"tempo_bicara\", \"confidence_score\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Roadmap\n",
        "- ‚úÖ **Phase 1**: LLM Assessment (kualitas, koherensi, relevansi)\n",
        "- üîß **Phase 2**: Audio Analysis Model (tempo_bicara) - *Coming Soon*\n",
        "- üîß **Phase 3**: Voice Analysis Model (confidence_score) - *Coming Soon*\n",
        "- üîß **Phase 4**: Video Analysis (eye contact, body language) - *Future*\n",
        "\n",
        "### Notes\n",
        "- **3 criteria** evaluated by LLM with real intelligence\n",
        "- **2 criteria** use static dummy values (will be replaced with specialized models)\n",
        "- Static values: `tempo_bicara=85`, `confidence_score=82`\n",
        "- Results saved automatically after transcription completes\n",
        "- **Original video files are deleted after transcription to save storage**\n",
        "- DeepL API key required for translation (free tier available)\n",
        "- Access via: `http://127.0.0.1:8888/results/{session_id}`\n",
        "\n",
        "### DeepL Setup\n",
        "1. Sign up at https://www.deepl.com/pro-api\n",
        "2. Get your free API key (500,000 chars/month)\n",
        "3. Set `DEEPL_API_KEY` in cell 4\n",
        "4. Restart kernel and run all cells"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05a63d6b2e3141b7ba80009416f089ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f2f59cf25b548e0991783c3381b3c80",
              "IPY_MODEL_0b8fc299e92445779f38301955f1c74a",
              "IPY_MODEL_4a58e52a06b442f7bcf947fd1f85a73f"
            ],
            "layout": "IPY_MODEL_85230263940e4be08284ca1d8d678ec7"
          }
        },
        "07a485be84e5483b922b56308c3d80d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d54a1e7f7ba54cb98b8d14a36a4a928f",
              "IPY_MODEL_29fdf0e668fb47ef97eb1f7f09949904",
              "IPY_MODEL_6a6c1b4f8f0d41c6afaf3659513c6da0"
            ],
            "layout": "IPY_MODEL_5e8345a4f35e4a899db2f2e517dacd2d"
          }
        },
        "08869c60ae074093bf22243dc4100d31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f44b35a30048b1bafa2d880d5ace72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0938548727284786a6a9a46b76e4ad5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8fc299e92445779f38301955f1c74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85706ed4d1db4286b509cce06903a6e0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919039342ed9486f93800e16866b3aee",
            "value": 1
          }
        },
        "100b1834bf164f86977e85126bd92565": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca42faa03b0404c9f170fc152b3514e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b77f6063438a44e7bc7dd9ffb86359ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f65e028e93ba4bf08fea59a6bac8293b",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "251e90b4abff4b89b861f8165b6e18e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283fd83ed74243c0b1b62db7ce2faf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29fdf0e668fb47ef97eb1f7f09949904": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f512c6f65bc444ff9197c242a0b6fb60",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_283fd83ed74243c0b1b62db7ce2faf94",
            "value": 1
          }
        },
        "32b0fd7dcc544c3e95297220fe9a5067": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3467fbff9a44b9aaceb48dac936835a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_251e90b4abff4b89b861f8165b6e18e9",
            "value": "‚Äá2.48M/?‚Äá[00:00&lt;00:00,‚Äá13.5MB/s]"
          }
        },
        "32dba1a9391d474c87df9e6481d2ea77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_100b1834bf164f86977e85126bd92565",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08f44b35a30048b1bafa2d880d5ace72",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "3adb986c4c564adaa7ef21adde0a400d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4017fad5c27c4edd8948951420ede424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a58e52a06b442f7bcf947fd1f85a73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca7487931944012bf8da019b51c49fd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_63ef4e4ea3c44d3da37ca1e2eb7efa08",
            "value": "‚Äá1.07M/?‚Äá[00:00&lt;00:00,‚Äá9.71MB/s]"
          }
        },
        "4cec7df9ce754d8f9071445537a2d71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32dba1a9391d474c87df9e6481d2ea77",
              "IPY_MODEL_f59b66bc5298449f8e0029126486309e",
              "IPY_MODEL_a69691951c614bb0a305bcf54740a454"
            ],
            "layout": "IPY_MODEL_986be52f627843029cade37a9af5b821"
          }
        },
        "4dc4963f0cb945bf9aebf6c08aa1596c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e047c2867f9472c94a4d17814613e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4e98f7ff057849fb8c299aa8cef0fb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc4963f0cb945bf9aebf6c08aa1596c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be695d1f31b042c9a6ec11af0f6c4166",
            "value": "model.bin:‚Äá100%"
          }
        },
        "4f2ecdc4ac6548f0bf96e5feacd7b7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ca42faa03b0404c9f170fc152b3514e",
              "IPY_MODEL_b48e6a73f6e345028987bbe195f0b96e",
              "IPY_MODEL_32b0fd7dcc544c3e95297220fe9a5067"
            ],
            "layout": "IPY_MODEL_de144b6aa6834f01aad4924324e09aae"
          }
        },
        "5e8345a4f35e4a899db2f2e517dacd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ef4e4ea3c44d3da37ca1e2eb7efa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67e4d643798f4b05b3ed99fdf3437605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ab95865fac4d6485d43f67e5107267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a6c1b4f8f0d41c6afaf3659513c6da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08869c60ae074093bf22243dc4100d31",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d52781d1e05b4442b9ac7c5b8f907dd5",
            "value": "‚Äá2.39k/?‚Äá[00:00&lt;00:00,‚Äá32.0kB/s]"
          }
        },
        "6dbfca3b9a2d41b0bf6e888f53869de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f2f59cf25b548e0991783c3381b3c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dbfca3b9a2d41b0bf6e888f53869de7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b73fb2727c6b4196a53544b470ee2e7e",
            "value": "vocabulary.json:‚Äá"
          }
        },
        "70c21b57c03b4acfa41ff6e0e55621c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71159fcbc0984d438368c9c1cd607b56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ba8a43171f4b25831cd99ede0a898b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ca7487931944012bf8da019b51c49fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85230263940e4be08284ca1d8d678ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85706ed4d1db4286b509cce06903a6e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "919039342ed9486f93800e16866b3aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "986be52f627843029cade37a9af5b821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aef69c2324a47cd9ac0b11ec8fbf1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a69691951c614bb0a305bcf54740a454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0938548727284786a6a9a46b76e4ad5d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_67e4d643798f4b05b3ed99fdf3437605",
            "value": "‚Äá340/340‚Äá[00:00&lt;00:00,‚Äá5.10kB/s]"
          }
        },
        "ab5afedbe0334e458df8344142362444": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4017fad5c27c4edd8948951420ede424",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_69ab95865fac4d6485d43f67e5107267",
            "value": "‚Äá3.09G/3.09G‚Äá[00:31&lt;00:00,‚Äá173MB/s]"
          }
        },
        "b48e6a73f6e345028987bbe195f0b96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e047c2867f9472c94a4d17814613e3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f359ce416141466fa4d44ca97b0d99ed",
            "value": 1
          }
        },
        "b73fb2727c6b4196a53544b470ee2e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b77f6063438a44e7bc7dd9ffb86359ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be695d1f31b042c9a6ec11af0f6c4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cad30de3a6514d1bb9af1d4147eda7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52781d1e05b4442b9ac7c5b8f907dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d54a1e7f7ba54cb98b8d14a36a4a928f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad30de3a6514d1bb9af1d4147eda7b8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_70c21b57c03b4acfa41ff6e0e55621c7",
            "value": "config.json:‚Äá"
          }
        },
        "d87be38d193b4fcb8005ad358fa1abbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3adb986c4c564adaa7ef21adde0a400d",
            "max": 3087284237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79ba8a43171f4b25831cd99ede0a898b",
            "value": 3087284237
          }
        },
        "d98113b7cd014433a10886bd6743b010": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e98f7ff057849fb8c299aa8cef0fb47",
              "IPY_MODEL_d87be38d193b4fcb8005ad358fa1abbc",
              "IPY_MODEL_ab5afedbe0334e458df8344142362444"
            ],
            "layout": "IPY_MODEL_f2f1521ff5944549b95f7a092dd5692f"
          }
        },
        "de144b6aa6834f01aad4924324e09aae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2f1521ff5944549b95f7a092dd5692f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3467fbff9a44b9aaceb48dac936835a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f359ce416141466fa4d44ca97b0d99ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f512c6f65bc444ff9197c242a0b6fb60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f59b66bc5298449f8e0029126486309e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71159fcbc0984d438368c9c1cd607b56",
            "max": 340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aef69c2324a47cd9ac0b11ec8fbf1ef",
            "value": 340
          }
        },
        "f65e028e93ba4bf08fea59a6bac8293b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
